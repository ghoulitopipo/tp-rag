{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydlAa2C-rSeh"
   },
   "source": [
    "# Lesson 6: Improve Agent's GPA\n",
    "\n",
    "In this lesson, you'll make two targeted changes to the agent:\n",
    "\n",
    "1. Adjust the planning prompt to include explicit goals, pre-conditions, and post-conditions for each step. This helps the executor understand the sub-goals it needs to reach.\n",
    "\n",
    "2. You will add inline evals so the agent receives feedback on when to do additional research. This provides the executor feedback on whether it's reaching its sub-goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtqgaXFU4_iv"
   },
   "source": [
    "```+---------------------------------------------------------------------------------------------------------+\n",
    "|                                    ARCHITECTURE DU SYSTEME MULTI-AGENTS                                 |\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "                                                |\n",
    "                                          [User Query]\n",
    "                                                |\n",
    "                                                v\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "|  1. PLANNER NODE (Le Cerveau)                                                                           |\n",
    "|  -----------------------------------------------------------------------------------------------------  |\n",
    "|  * Fonction : `planner_node(state)`                                                                     |\n",
    "|  * Prompt   : `patched_plan_prompt` (qui utilise RECURSION_LIMIT pour gÃ©rer le budget)                  |\n",
    "|  * Sortie   : GÃ©nÃ¨re un plan JSON (Step 1, Step 2, ...) ajoutÃ© au `state['plan']`                       |\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "                                                |\n",
    "                                                v\n",
    "          +---------------------------------------------------------------------------+\n",
    "          |                                                                           |\n",
    "          |   2. EXECUTOR NODE (Le Chef d'Orchestre)    <-------------------------+   |\n",
    "          |   -----------------------------------------------------------------   |   |\n",
    "          |   * Fonction : `executor_node(state)`                                 |   |\n",
    "          |   * Logique  : Lit le `state['plan']`, vÃ©rifie `current_step`.        |   |\n",
    "          |                DÃ©cide quel agent appeler (Routing).                   |   |\n",
    "          |                GÃ¨re les drapeaux `replan` et `previous_step_failed`.  |   |\n",
    "          |                                                                           |\n",
    "          +----------------------+----------------------+-------------------------+---+\n",
    "                                 |                      |                         |\n",
    "        +------------------------+                      |                         |\n",
    "        | (Route: \"cortex_researcher\")                  | (Route: \"web_...\")      | (Route: \"chart_...\")\n",
    "        v                                               v                         v\n",
    "+------------------------------------+    +-----------------------------+    +-----------------------------+\n",
    "| 3a. CORTEX RESEARCH NODE           |    | 3b. WEB RESEARCH NODE       |    | 3c. CHART GENERATOR NODE    |\n",
    "| ---------------------------------- |    | --------------------------- |    | --------------------------- |\n",
    "| * Func: `cortex_agents_research_...|    | * Func: `web_research_node` |    | * Func: `chart_node`        |\n",
    "| * Outils :                         |    | * Agent : ReAct Agent       |    | * Agent : ReAct Agent       |\n",
    "|   - `wikipedia_rag_tool`           |    | * Outil : `tavily_tool`     |    | * Outil : `PythonREPL`      |\n",
    "|   - `wikidata_sparql_tool`         |    |                             |    |                             |\n",
    "| * Note: Utilise `_cortex_llm_...`  |    |                             |    |                             |\n",
    "+-----------------+------------------+    +--------------+--------------+    +--------------+--------------+\n",
    "                  |                                      |                                  |\n",
    "                  |                                      |                                  |\n",
    "                  +-------------------+------------------+----------------------------------+\n",
    "                                      |\n",
    "                                      | (RÃ©sultat de l'Ã©tape / Demande de Replanification)\n",
    "                                      | Retour vers Executor\n",
    "                                      |\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "|                                     CONDITION DE FIN                                                    |\n",
    "| Si (toutes les Ã©tapes finies) OU (replanification impossible) OU (remaining_steps <= 0)                 |\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "                                                |\n",
    "                                                v\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "|  4. SYNTHESIZER NODE (Le RÃ©dacteur)                                                                     |\n",
    "|  -----------------------------------------------------------------------------------------------------  |\n",
    "|  * Fonction : `synthesizer_node(state)`                                                                 |\n",
    "|  * Prompt   : `final_answer_prompt`                                                                     |\n",
    "|  * EntrÃ©e   : Prend tout l'historique des messages et des rÃ©sultats d'outils.                           |\n",
    "|  * Sortie   : RÃ©ponse finale structurÃ©e pour l'utilisateur.                                             |\n",
    "+---------------------------------------------------------------------------------------------------------+\n",
    "                                                |\n",
    "                                                v\n",
    "                                          [Final Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZU50-durbyM",
    "outputId": "70ccf6ce-d226-4d05-e419-ce8e3cde8ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed.\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Install Dependencies & Setup (will kill 1st time then, re-launch)\n",
    "import os, sys, time\n",
    "if os.path.exists(\".lib_installed\"):\n",
    "    print(\"Dependencies installed.\")\n",
    "else:\n",
    "  !pip install -q \\\n",
    "      langchain \\\n",
    "      langchain-core \\\n",
    "      langchain-community \\\n",
    "      langchain-openai \\\n",
    "      langchain-groq \\\n",
    "      langchain-experimental \\\n",
    "      langgraph \\\n",
    "      langchain-huggingface \\\n",
    "      chromadb \\\n",
    "      pypdf \\\n",
    "      duckdb \\\n",
    "      duckduckgo-search \\\n",
    "      trulens-core trulens-providers-openai trulens-apps-langgraph trulens-dashboard \\\n",
    "      opentelemetry-sdk nest-asyncio2 openinference-instrumentation-langchain arize-phoenix uvicorn \\\n",
    "      python-dotenv\n",
    "\n",
    "  with open(\".lib_installed\", \"w\") as f: f.write(\"Installation OK\")\n",
    "\n",
    "import nest_asyncio2 as nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qenKYEtHrq0V"
   },
   "outputs": [],
   "source": [
    "# @title 2. Central Configuration & Secrets\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"gsk_XXX\"\n",
    "\n",
    "# Utiliser Groq avec modeles gratuits\n",
    "os.environ[\"MODEL_EXECUTOR\"] = \"llama-3.1-8b-instant\"\n",
    "os.environ[\"MODEL_REASONING\"] = \"llama-3.1-8b-instant\" \n",
    "os.environ[\"MODEL_EVAL\"] = \"llama-3.1-8b-instant\"\n",
    "\n",
    "# TruLens setup\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
    "\n",
    "print(\"Configuration: Groq API + DuckDuckGo Search (100% gratuit et open-source)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ttQS-dwMOcj"
   },
   "source": [
    "# Setup a rate limiter to ensure to enjoy free Groq API :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1blwT4ZL-mq",
    "outputId": "edcb8739-93a5-4ee4-9f13-d5f72d39ce12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¡ï¸ Groq Armor activÃ© : 20 RPM + Auto-Retry sur 429 (LangChain & TruLens protÃ©gÃ©s)\n"
     ]
    }
   ],
   "source": [
    "# @title 3. Rate Limiter pour Groq API gratuite\n",
    "from typing import Callable, Any\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "class GroqRateLimiter:\n",
    "    def __init__(self, requests_per_minute: int = 30, tokens_per_minute: int = 14400):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "        self.request_times = []\n",
    "        self.token_count = 0\n",
    "        self.token_reset_time = time.time() + 60\n",
    "    \n",
    "    def wait_if_needed(self, estimated_tokens: int = 500):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Reset token counter chaque minute\n",
    "        if current_time >= self.token_reset_time:\n",
    "            self.token_count = 0\n",
    "            self.token_reset_time = current_time + 60\n",
    "        \n",
    "        # Nettoyer les anciennes requetes\n",
    "        self.request_times = [t for t in self.request_times if current_time - t < 60]\n",
    "        \n",
    "        # Verifier limites requetes\n",
    "        if len(self.request_times) >= self.requests_per_minute:\n",
    "            wait_time = 60 - (current_time - self.request_times[0])\n",
    "            if wait_time > 0:\n",
    "                print(f\"Rate limit: attente {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                return self.wait_if_needed(estimated_tokens)\n",
    "        \n",
    "        # Verifier limites tokens\n",
    "        if self.token_count + estimated_tokens > self.tokens_per_minute:\n",
    "            wait_time = self.token_reset_time - current_time\n",
    "            if wait_time > 0:\n",
    "                print(f\"Token limit: attente {wait_time:.1f}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                self.token_count = 0\n",
    "                self.token_reset_time = time.time() + 60\n",
    "        \n",
    "        self.request_times.append(current_time)\n",
    "        self.token_count += estimated_tokens\n",
    "\n",
    "rate_limiter = GroqRateLimiter(requests_per_minute=30, tokens_per_minute=14400)\n",
    "\n",
    "def rate_limited_llm_call(func: Callable) -> Callable:\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        rate_limiter.wait_if_needed(estimated_tokens=500)\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "print(\"Rate limiter Groq configure: 30 req/min, 14400 tokens/min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UGjO3Nh3OTWH",
    "outputId": "e1835730-44e8-4f22-ab1b-b1a28e6db553"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "ERROR [opentelemetry.sdk._shared_internal] Exception while exporting Span.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 667, in send\n",
      "    if isinstance(e.reason, ResponseError):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 157, in _export\n",
      "    resp = self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 700, in send\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 494, in request\n",
      "    self.endheaders()\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.12/http/client.py\", line 1037, in send\n",
      "    self.connect()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 325, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 667, in send\n",
      "    if isinstance(e.reason, ResponseError):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/sdk/_shared_internal/__init__.py\", line 179, in _export\n",
      "    self._exporter.export(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 182, in export\n",
      "    resp = self._export(serialized_data, deadline_sec - time())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 165, in _export\n",
      "    resp = self._session.post(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 700, in send\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "WARNI [phoenix.session.session] âš ï¸ PHOENIX_COLLECTOR_ENDPOINT is set to http://localhost:6002/v1/traces.\n",
      "âš ï¸ This means that traces will be sent to the collector endpoint and not this app.\n",
      "âš ï¸ If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
      "âš ï¸ You will need to restart your notebook to apply this change.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit https://nelnk6aryaa9-496ff2e9c6d22116-6002-colab.googleusercontent.com/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "ðŸš€ Phoenix UI is ready at: https://nelnk6aryaa9-496ff2e9c6d22116-6002-colab.googleusercontent.com/\n"
     ]
    },
    {
     "data": {
      "application/javascript": "(async (port, path, width, height, cache, element) => {\n    if (!google.colab.kernel.accessAllowed && !cache) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port, {cache});\n    const iframe = document.createElement('iframe');\n    iframe.src = new URL(path, url).toString();\n    iframe.height = height;\n    iframe.width = width;\n    iframe.style.border = 0;\n    iframe.allow = [\n        'accelerometer',\n        'autoplay',\n        'camera',\n        'clipboard-read',\n        'clipboard-write',\n        'gyroscope',\n        'magnetometer',\n        'microphone',\n        'serial',\n        'usb',\n        'xr-spatial-tracking',\n    ].join('; ');\n    element.appendChild(iframe);\n  })(6002, \"/\", \"100%\", 1000, false, window.element)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Initiate ðŸš€ Phoenix monitoring of Langchain / LangGraph\n",
    "import phoenix as px\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "import os, time\n",
    "from google.colab import output\n",
    "\n",
    "os.environ[\"PHOENIX_PORT\"] = \"6002\" # Petite sÃ©curitÃ© pour Ã©viter le conflit de ports si vous relancez plusieurs fois\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:\" + os.environ[\"PHOENIX_PORT\"] + \"/v1/traces\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = \"langgraph-data-toulon\"\n",
    "\n",
    "try:\n",
    "    phoenix_session = px.launch_app() # 1. Lancer l'UI locale\n",
    "    time.sleep(5) # Give it time to spin up\n",
    "    print(f\"ðŸš€ Phoenix UI is ready at: {phoenix_session.url}\")\n",
    "    # try: # L'instumentation est dÃ©placÃ©e aprÃ¨s l'initialisation de TruGraph pour se brancher dessus pour permettre de partager le flux OTEL\n",
    "    #     LangChainInstrumentor().instrument() # 2. Activer l'instrumentation\n",
    "    #     print(\"âœ… Instrumentation activÃ©e.\") # LangChainInstrumentor capture aussi les noeuds LangGraph de base\n",
    "    # except Exception as e:\n",
    "    #     print(f\"âš ï¸ Erreur d'instrumentation (peut-Ãªtre dÃ©jÃ  active): {e}\")\n",
    "    output.serve_kernel_port_as_iframe(os.environ[\"PHOENIX_PORT\"], height=1000) # Cela ouvre une fenÃªtre directement dans le notebook\n",
    "except Exception as e:\n",
    "    print(f\"Erreur au lancement: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV6A4xnNrweW"
   },
   "source": [
    "CrÃ©ation de prompts.py, helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IylUwZSkrsRf"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#%%writefile prompts.py\n",
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING, Dict, Any\n",
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from helper import State\n",
    "\n",
    "def get_agent_descriptions() -> Dict[str, Dict[str, str]]:\n",
    "    return {\n",
    "        \"web_researcher\": {\n",
    "            \"name\": \"Web Researcher\",\n",
    "            \"capability\": \"DuckDuckGo search for general internet knowledge, current events, real-time data (gratuit, sans API key)\",\n",
    "            \"use_when\": \"Question requires external web knowledge or recent information not in local documents\",\n",
    "            \"limitations\": \"Cannot access local documents or structured data. Results may be less structured than Tavily.\",\n",
    "            \"output_format\": \"Web search results with snippets\",\n",
    "        },\n",
    "        \"cortex_researcher\": {\n",
    "            \"name\": \"Cortex Researcher\",\n",
    "            \"capability\": \"Acces aux documents locaux via RAG vectoriel ET requetes SQL analytiques via DuckDB\",\n",
    "            \"use_when\": \"Questions sur les documents indexes (RAG) OU requetes analytiques (SQL: comptages, statistiques, listes)\",\n",
    "            \"limitations\": \"Limite aux documents indexes et donnees structurees locales\",\n",
    "            \"output_format\": \"Extraits de documents (RAG) ou tableaux de donnees (SQL)\",\n",
    "        },\n",
    "        \"chart_generator\": {\n",
    "            \"name\": \"Chart Generator\",\n",
    "            \"capability\": \"Python code execution for data visualization using matplotlib, plotly, pandas, numpy\",\n",
    "            \"use_when\": \"User explicitly requests visualization/chart/graph OR analysis results need visual representation\",\n",
    "            \"limitations\": \"Cannot search for information, only visualize provided data\",\n",
    "            \"output_format\": \"Chart saved to file with path\",\n",
    "        },\n",
    "        \"chart_summarizer\": {\n",
    "            \"name\": \"Chart Summarizer\",\n",
    "            \"capability\": \"Generates natural language descriptions of charts\",\n",
    "            \"use_when\": \"After chart_generator creates a visualization that needs textual explanation\",\n",
    "            \"limitations\": \"Only works after chart_generator, cannot create charts\",\n",
    "            \"output_format\": \"Concise 1-3 sentence description\",\n",
    "        },\n",
    "        \"synthesizer\": {\n",
    "            \"name\": \"Synthesizer\",\n",
    "            \"capability\": \"Combines information from multiple sources into coherent final answer\",\n",
    "            \"use_when\": \"After all research/visualization tasks complete, to produce final unified response\",\n",
    "            \"limitations\": \"Cannot gather new information, only synthesizes existing results\",\n",
    "            \"output_format\": \"Natural language summary with citations\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def _get_enabled_agents(state: State) -> list:\n",
    "    enabled = state.get(\"enabled_agents\")\n",
    "    if enabled is None or len(enabled) == 0:\n",
    "        enabled = [\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"]\n",
    "    if \"planner\" not in enabled:\n",
    "        enabled.append(\"planner\")\n",
    "    return enabled\n",
    "\n",
    "def format_agent_list(state: State) -> str:\n",
    "    enabled = _get_enabled_agents(state)\n",
    "    descriptions = get_agent_descriptions()\n",
    "    formatted = []\n",
    "\n",
    "    for agent_name in sorted(enabled):\n",
    "        if agent_name == \"planner\":\n",
    "            continue\n",
    "        desc = descriptions.get(agent_name, {})\n",
    "        formatted.append(\n",
    "            f\"- **{desc.get('name', agent_name)}** (`{agent_name}`): {desc.get('capability', 'No description')}\"\n",
    "        )\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def format_agent_guidelines_for_planner(state: State) -> str:\n",
    "    enabled = _get_enabled_agents(state)\n",
    "    descriptions = get_agent_descriptions()\n",
    "    guidelines = []\n",
    "\n",
    "    for agent_name in sorted(enabled):\n",
    "        if agent_name == \"planner\":\n",
    "            continue\n",
    "        desc = descriptions.get(agent_name, {})\n",
    "        guidelines.append(\n",
    "            f\"- Use `{agent_name}` when: {desc.get('use_when', 'appropriate')}\\n\"\n",
    "            f\"  Limitations: {desc.get('limitations', 'none noted')}\"\n",
    "        )\n",
    "    return \"\\n\".join(guidelines)\n",
    "\n",
    "def format_agent_guidelines_for_executor(state: State) -> str:\n",
    "    enabled = _get_enabled_agents(state)\n",
    "    descriptions = get_agent_descriptions()\n",
    "    guidelines = []\n",
    "\n",
    "    for agent_name in sorted(enabled):\n",
    "        if agent_name == \"planner\":\n",
    "            continue\n",
    "        desc = descriptions.get(agent_name, {})\n",
    "        guidelines.append(\n",
    "            f\"- `{agent_name}`: {desc.get('output_format', 'standard output')}\\n\"\n",
    "            f\"  Use when: {desc.get('use_when', 'appropriate')}\"\n",
    "        )\n",
    "    return \"\\n\".join(guidelines)\n",
    "\n",
    "def plan_prompt(state: State) -> HumanMessage:\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    replan_flag = state.get(\"replan_flag\", False)\n",
    "    replan_reason = state.get(\"last_reason\", \"\")\n",
    "    prior_plan = state.get(\"plan\", {})\n",
    "\n",
    "    planner_agent_enum = \"|\".join([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']])\n",
    "    agent_list = format_agent_list(state)\n",
    "    agent_guidelines = format_agent_guidelines_for_planner(state)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are the **Planner** in a multi-agent system that orchestrates a team of specialized agents.\n",
    "\n",
    "        **Available Agents:**\n",
    "\n",
    "        {agent_list}\n",
    "\n",
    "        Return **ONLY** valid JSON (no markdown, no explanations) in this form:\n",
    "\n",
    "        {{\n",
    "        \"1\": {{\n",
    "            \"agent\": \"{planner_agent_enum}\",\n",
    "            \"action\": \"string\",\n",
    "        }},\n",
    "        \"2\": {{ ... }},\n",
    "        \"3\": {{ ... }}\n",
    "        }}\n",
    "\n",
    "        Guidelines:\n",
    "        {agent_guidelines}\n",
    "        \"\"\"\n",
    "\n",
    "    if replan_flag:\n",
    "        prompt += f\"\"\"\n",
    "        The current plan needs revision because: {replan_reason}\n",
    "\n",
    "        Current plan:\n",
    "        {json.dumps(prior_plan, indent=2)}\n",
    "\n",
    "        When replanning:\n",
    "        - Focus on UNBLOCKING the workflow rather than perfecting it.\n",
    "        - Only modify steps that are truly preventing progress.\n",
    "        - Prefer simpler, more achievable alternatives over complex rewrites.\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        prompt += \"\\nGenerate a new plan from scratch.\"\n",
    "\n",
    "    prompt += f'\\nUser query: \"{user_query}\"'\n",
    "\n",
    "    return HumanMessage(content=prompt)\n",
    "\n",
    "def executor_prompt(state: State) -> HumanMessage:\n",
    "    step = int(state.get(\"current_step\", 0))\n",
    "    latest_plan: Dict[str, Any] = state.get(\"plan\") or {}\n",
    "    plan_block: Dict[str, Any] = latest_plan.get(str(step), {})\n",
    "    max_replans    = 2\n",
    "    attempts       = (state.get(\"replan_attempts\", {}) or {}).get(step, 0)\n",
    "\n",
    "    executor_guidelines = format_agent_guidelines_for_executor(state)\n",
    "    plan_agent = plan_block.get(\"agent\", \"web_researcher\")\n",
    "\n",
    "    messages_tail = (state.get(\"messages\") or [])[-4:]\n",
    "\n",
    "    executor_prompt = f\"\"\"\n",
    "        **IMPORTANT:** Respond **ONLY** with a valid JSON object. Do NOT include any additional text, explanation, or conversational phrases, such as \"FINAL ANSWER\".\n",
    "\n",
    "        {{\n",
    "        \"replan\": <true|false>,\n",
    "        \"goto\": \"<{ '|'.join([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']) }>\",\n",
    "        \"reason\": \"<1 sentence>\",\n",
    "        \"query\": \"<text>\"\n",
    "        }}\n",
    "\n",
    "        You are the **executor** in a multi-agent system with these agents:\n",
    "        `{ '`, `'.join(sorted(set([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']))) }`.\n",
    "\n",
    "        **Tasks**\n",
    "        1. Decide if the current plan needs revision.  â†’ `\"replan_flag\": true|false`\n",
    "        2. Decide which agent to run next.             â†’ `\"goto\": \"<agent_name>\"`\n",
    "        3. Give oneâ€‘sentence justification.            â†’ `\"reason\": \"<text>\"`\n",
    "        4. Write the exact question that the chosen agent should answer\n",
    "                                                    â†’ \"query\": \"<text>\"\n",
    "\n",
    "        **Guidelines**\n",
    "        {executor_guidelines}\n",
    "        - After **{max_replans}** failed replans for the same step, move on.\n",
    "        - If you *just replanned* (replan_flag is true) let the assigned agent try before\n",
    "        requesting another replan.\n",
    "\n",
    "        **PRIORITIZE FORWARD PROGRESS:** Only replan if the current step is completely blocked.\n",
    "        1. If any reasonable data was obtained that addresses the step's core goal, set `\"replan\": false` and proceed.\n",
    "        2. Set `\"replan\": true` **only if** ALL of these conditions are met:\n",
    "        â€¢ The step has produced zero useful information\n",
    "        â€¢ The missing information cannot be approximated or obtained by remaining steps\n",
    "        â€¢ `attempts < {max_replans}`\n",
    "        3. When `attempts == {max_replans}`, always move forward (`\"replan\": false`).\n",
    "\n",
    "        ### Decide `\"goto\"`\n",
    "        - If `\"replan\": true` â†’ `\"goto\": \"planner\"`.\n",
    "        - If current step has made reasonable progress â†’ move to next step's agent.\n",
    "        - Otherwise execute the current step's assigned agent (`{plan_agent}`).\n",
    "\n",
    "        ### Build `\"query\"`\n",
    "        Write a clear, standalone instruction for the chosen agent. If the chosen agent\n",
    "        is `web_researcher` or `cortex_researcher`, the query should be a standalone question,\n",
    "        written in plain english, and answerable by the agent.\n",
    "\n",
    "        Ensure that the query uses consistent language as the user's query.\n",
    "\n",
    "        Context you can rely on\n",
    "        - User query ..............: {state.get(\"user_query\")}\n",
    "        - Current step index ......: {step}\n",
    "        - Current plan step .......: {plan_block}\n",
    "        - Justâ€‘replanned flag .....: {state.get(\"replan_flag\")}\n",
    "        - Previous messages .......: {messages_tail}\n",
    "        \"\"\"\n",
    "\n",
    "    return HumanMessage(\n",
    "        content=executor_prompt\n",
    "    )\n",
    "\n",
    "def agent_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH_8GCeQBOWb"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#%%writefile helper.py\n",
    "from __future__ import annotations\n",
    "# pyright: reportMissingImports=false, reportMissingTypeStubs=false, reportIncompatibleMethodOverride=false\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Valid config keys have changed in V2\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"WARNING! response_format is not default parameter\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"pkg_resources is deprecated as an API.*\", category=UserWarning, module=r\"^munch$\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Annotated, Literal, Optional, List, Dict, Any, Type\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from pydantic import BaseModel, PrivateAttr\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState, START, StateGraph, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from trulens.core import Feedback, Select\n",
    "from trulens.core.feedback.selector import Selector\n",
    "from trulens.providers.openai import OpenAI\n",
    "import numpy as np\n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def select_context(output):\n",
    "    return [m.content for m in output.get('messages', []) if getattr(m, 'name', '') in ['web_researcher', 'cortex_researcher']]\n",
    "\n",
    "def select_plan_text(output):\n",
    "    for m in output.get('messages', []):\n",
    "        if getattr(m, 'name', '') in ['initial_plan', 'replan']: return m.content\n",
    "    return \"\"\n",
    "\n",
    "def select_user_query(output):\n",
    "    return output.get(\"user_query\", \"\")\n",
    "\n",
    "def select_final_answer(output):\n",
    "    return output.get(\"final_answer\", \"\")\n",
    "\n",
    "def select_all(data):\n",
    "    return data\n",
    "\n",
    "class State(MessagesState):\n",
    "    enabled_agents: Optional[List[str]]\n",
    "    plan: Optional[Dict[str, Dict[str, Any]]]\n",
    "    user_query: Optional[str]\n",
    "    current_step: int\n",
    "    replan_flag: Optional[bool]\n",
    "    last_reason: Optional[str]\n",
    "    replan_attempts: Optional[Dict[int, int]]\n",
    "    agent_query: Optional[str]\n",
    "    remaining_steps: RemainingSteps\n",
    "\n",
    "MAX_REPLANS = 2\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. You will be used to execute python code\n",
    "    that generates charts. Only print the chart once.\n",
    "    This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = (\n",
    "        f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    )\n",
    "    return (\n",
    "        result_str\n",
    "        + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "reasoning_llm = ChatGroq(\n",
    "    model=os.environ[\"MODEL_REASONING\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "@instrument(attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        \"retrieved_plan\": json.dumps(ret.update.get(\"plan\", {})),\n",
    "        \"retrieved_query\": args[0].get(\"user_query\") or args[0].get(\"messages\", [HumanMessage(content=\"\")])[0].content})\n",
    "def planner_node(state: State) \\\n",
    "        -> \"Command[Literal['executor']]\":\n",
    "    from prompts import plan_prompt\n",
    "    reply = reasoning_llm.invoke([plan_prompt(state)])\n",
    "    text = reply.content\n",
    "\n",
    "    import re\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        text = json_match.group()\n",
    "\n",
    "    plan_dict = json.loads(text)\n",
    "\n",
    "    is_replan = bool(state.get(\"replan_flag\"))\n",
    "    step = state.get(\"current_step\", 0)\n",
    "\n",
    "    if is_replan:\n",
    "        attempts_map = state.get(\"replan_attempts\", {}) or {}\n",
    "        attempts_map[step] = attempts_map.get(step, 0) + 1\n",
    "        tag = \"replan\"\n",
    "    else:\n",
    "        attempts_map = {}\n",
    "        tag = \"initial_plan\"\n",
    "\n",
    "    new_msg = HumanMessage(content=json.dumps(plan_dict, indent=2), name=tag)\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"plan\": plan_dict,\n",
    "            \"messages\": [new_msg],\n",
    "            \"replan_attempts\": attempts_map,\n",
    "        },\n",
    "        goto=\"executor\",\n",
    "    )\n",
    "\n",
    "@instrument(attributes=lambda ret, exception, *args, **kwargs: {\n",
    "    \"retrieved_execution\": ret.update.get(\"messages\", [HumanMessage(content=\"\")])[-1].content if hasattr(ret, 'update') else \"\"})\n",
    "def executor_node(state: State) \\\n",
    "        -> Command[Literal[\"planner\", \"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"]]:\n",
    "    from prompts import executor_prompt\n",
    "    reply = reasoning_llm.invoke([executor_prompt(state)])\n",
    "    text = reply.content\n",
    "\n",
    "    import re\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        text = json_match.group()\n",
    "\n",
    "    decision = json.loads(text)\n",
    "\n",
    "    replan_flag = decision.get(\"replan\", False)\n",
    "    goto = decision.get(\"goto\", \"planner\")\n",
    "    reason = decision.get(\"reason\", \"\")\n",
    "    query = decision.get(\"query\", \"\")\n",
    "\n",
    "    step = state.get(\"current_step\", 0)\n",
    "    plan = state.get(\"plan\", {})\n",
    "    planned_agent = plan.get(str(step), {}).get(\"agent\", \"web_researcher\")\n",
    "\n",
    "    updates = {\n",
    "        \"messages\": [HumanMessage(content=reason, name=\"executor\")],\n",
    "        \"agent_query\": query\n",
    "    }\n",
    "    updates[\"current_step\"] = step + 1 if goto == planned_agent else step\n",
    "    updates[\"replan_flag\"] = False\n",
    "    return Command(update=updates, goto=goto)\n",
    "\n",
    "# RAG Local - Remplace Wikipedia/Wikidata\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "persist_directory = \"chroma_db\"\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "simple_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "store_parent = InMemoryStore()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store_parent,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "data_root = Path('data')\n",
    "text_exts = {'.txt', '.tex', '.bib'}\n",
    "pdf_ext = '.pdf'\n",
    "documents = []\n",
    "data_files = []\n",
    "\n",
    "if data_root.exists():\n",
    "    for p in sorted(data_root.rglob('*')):\n",
    "        if p.is_file() and p.suffix.lower() in text_exts.union({pdf_ext}):\n",
    "            data_files.append(str(p))\n",
    "\n",
    "for fp in data_files:\n",
    "    p = Path(fp)\n",
    "    if p.suffix.lower() in text_exts:\n",
    "        try:\n",
    "            loader = TextLoader(str(p), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur chargement {p}: {e}\")\n",
    "    elif p.suffix.lower() == pdf_ext:\n",
    "        try:\n",
    "            pdf_loader = PyPDFLoader(str(p))\n",
    "            docs = pdf_loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur chargement {p}: {e}\")\n",
    "\n",
    "if documents:\n",
    "    parent_retriever.add_documents(documents)\n",
    "    print(f\"Documents indexes: {len(documents)}\")\n",
    "\n",
    "@tool\n",
    "def local_rag_tool(query: str, use_parent: bool = False):\n",
    "    \"\"\"\n",
    "    Recherche dans la base documentaire locale.\n",
    "    Args:\n",
    "        query: Question utilisateur\n",
    "        use_parent: Si True, utilise ParentDocumentRetriever (contexte large)\n",
    "    \"\"\"\n",
    "    retriever = parent_retriever if use_parent else simple_retriever\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    if not docs:\n",
    "        return \"Aucun document trouve.\"\n",
    "    \n",
    "    output = f\"Resultats pour '{query}':\\n\\n\"\n",
    "    for i, doc in enumerate(docs[:3], 1):\n",
    "        source = Path(doc.metadata.get('source', 'Unknown')).name\n",
    "        output += f\"[{i}] Source: {source}\\n{doc.page_content[:500]}...\\n\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# SQL Local - DuckDB\n",
    "import duckdb\n",
    "\n",
    "duckdb_path = \"research_data.duckdb\"\n",
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS research_papers (\n",
    "        paper_id INTEGER PRIMARY KEY,\n",
    "        title VARCHAR,\n",
    "        year INTEGER,\n",
    "        topic VARCHAR,\n",
    "        file_type VARCHAR,\n",
    "        file_path VARCHAR\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "doc_metadata = []\n",
    "for idx, doc in enumerate(documents[:50], start=1):\n",
    "    source_path = doc.metadata.get('source', 'Unknown')\n",
    "    file_name = Path(source_path).name\n",
    "    \n",
    "    title = file_name.replace('.txt', '').replace('.tex', '').replace('.bib', '').replace('_', ' ')\n",
    "    \n",
    "    if 'multi' in file_name.lower() and 'agent' in file_name.lower():\n",
    "        topic = 'Multi-Agent Systems'\n",
    "    elif 'blood' in file_name.lower() or 'pressure' in file_name.lower():\n",
    "        topic = 'Medical'\n",
    "    elif 'security' in file_name.lower() or 'smart' in file_name.lower() or 'grid' in file_name.lower():\n",
    "        topic = 'Cybersecurity'\n",
    "    elif 'quantum' in file_name.lower():\n",
    "        topic = 'Quantum Computing'\n",
    "    elif 'economic' in file_name.lower() or 'inflation' in file_name.lower():\n",
    "        topic = 'Economics'\n",
    "    elif 'container' in file_name.lower() or 'stowage' in file_name.lower():\n",
    "        topic = 'Logistics'\n",
    "    else:\n",
    "        topic = 'General'\n",
    "    \n",
    "    year = 2020 + (idx % 5)\n",
    "    file_type = Path(source_path).suffix.replace('.', '')\n",
    "    \n",
    "    doc_metadata.append((idx, title, year, topic, file_type, source_path))\n",
    "\n",
    "if doc_metadata:\n",
    "    con.executemany(\n",
    "        \"INSERT OR IGNORE INTO research_papers VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "        doc_metadata\n",
    "    )\n",
    "    print(f\"DuckDB initialized with {len(doc_metadata)} papers\")\n",
    "\n",
    "@tool\n",
    "def duckdb_sql_tool(natural_query: str):\n",
    "    \"\"\"\n",
    "    Execute des requetes SQL sur la base DuckDB locale.\n",
    "    Traduit automatiquement du langage naturel en SQL.\n",
    "    Utilise pour des questions analytiques: comptages, statistiques, comparaisons.\n",
    "    \"\"\"\n",
    "    translator_llm = ChatGroq(model=os.environ[\"MODEL_EXECUTOR\"], temperature=0)\n",
    "    \n",
    "    sql_prompt = f\"\"\"Traduis cette question en SQL pour DuckDB.\n",
    "\n",
    "Table disponible:\n",
    "- research_papers (paper_id INTEGER, title VARCHAR, year INTEGER, topic VARCHAR, file_type VARCHAR, file_path VARCHAR)\n",
    "\n",
    "Topics disponibles: Multi-Agent Systems, Medical, Cybersecurity, Quantum Computing, Economics, Logistics, General\n",
    "\n",
    "Question: {natural_query}\n",
    "\n",
    "Retourne UNIQUEMENT le code SQL sans markdown ni explications.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = translator_llm.invoke(sql_prompt)\n",
    "        sql_query = response.content.strip()\n",
    "        \n",
    "        if '```' in sql_query:\n",
    "            sql_query = re.search(r'```(?:sql)?\\s*(.*?)\\s*```', sql_query, re.DOTALL)\n",
    "            if sql_query:\n",
    "                sql_query = sql_query.group(1).strip()\n",
    "        \n",
    "        result = con.execute(sql_query).fetchdf()\n",
    "        \n",
    "        if result.empty:\n",
    "            return \"Aucun resultat trouve.\"\n",
    "        \n",
    "        return f\"SQL: {sql_query}\\n\\nResultats:\\n{result.to_string(index=False, max_rows=20)}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erreur SQL: {str(e)}\"\n",
    "\n",
    "llm = ChatGroq(model=os.environ[\"MODEL_EXECUTOR\"], temperature=0.7)\n",
    "_cortex_llm_with_tools = llm.bind_tools([local_rag_tool, duckdb_sql_tool])\n",
    "\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            ret.update[\"messages\"][-1].content\n",
    "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def cortex_agents_research_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[\"executor\"]]:\n",
    "    query = state.get(\"agent_query\", state.get(\"user_query\", \"\"))\n",
    "    \n",
    "    use_parent = \"large context\" in query.lower() or len(query.split()) > 15\n",
    "    \n",
    "    prompt = f\"\"\"Tu disposes de 2 outils:\n",
    "1. local_rag_tool: pour recherche semantique dans le contenu des documents\n",
    "2. duckdb_sql_tool: pour requetes analytiques (comptages, statistiques, listes)\n",
    "\n",
    "Choisis l'outil approprie pour: {query}\n",
    "\n",
    "Si la question demande des statistiques/comptages/comparaisons: utilise duckdb_sql_tool\n",
    "Si la question demande du contenu/explications: utilise local_rag_tool\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = _cortex_llm_with_tools.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            results = []\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call.get(\"name\", \"\")\n",
    "                tool_args = tool_call.get(\"args\", {})\n",
    "                \n",
    "                try:\n",
    "                    if tool_name == \"local_rag_tool\":\n",
    "                        tool_query = tool_args.get(\"query\", query)\n",
    "                        use_parent_arg = tool_args.get(\"use_parent\", use_parent)\n",
    "                        result = local_rag_tool.invoke({\n",
    "                            \"query\": tool_query,\n",
    "                            \"use_parent\": use_parent_arg\n",
    "                        })\n",
    "                    elif tool_name == \"duckdb_sql_tool\":\n",
    "                        tool_query = tool_args.get(\"natural_query\", query)\n",
    "                        result = duckdb_sql_tool.invoke({\"natural_query\": tool_query})\n",
    "                    else:\n",
    "                        result = f\"Outil inconnu: {tool_name}\"\n",
    "                except Exception as tool_error:\n",
    "                    result = f\"Erreur outil {tool_name}: {str(tool_error)}\"\n",
    "                \n",
    "                results.append(f\"=== {tool_name} ===\\n{result}\")\n",
    "            \n",
    "            final_content = \"\\n\\n\".join(results)\n",
    "        else:\n",
    "            final_content = response.content or f\"Pas de resultat pour: {query}\"\n",
    "    except Exception as e:\n",
    "        final_content = f\"Erreur recherche: {str(e)}\"\n",
    "    \n",
    "    return Command(\n",
    "        update={\"messages\": [HumanMessage(content=final_content, name=\"cortex_researcher\")]},\n",
    "        goto=\"executor\",\n",
    "    )\n",
    "\n",
    "# DuckDuckGo Search (gratuit, sans API key)\n",
    "ddg_search = DuckDuckGoSearchRun()\n",
    "\n",
    "@instrument(\n",
    "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "    attributes=lambda ret, exception, *args, **kwargs: {\n",
    "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None,\n",
    "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
    "            ret.update[\"messages\"][-1].content\n",
    "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
    "    },\n",
    ")\n",
    "def duckduckgo_search_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[\"executor\"]]:\n",
    "    from prompts import agent_system_prompt\n",
    "    query = state.get(\"agent_query\", state.get(\"user_query\", \"\"))\n",
    "    try:\n",
    "        # DuckDuckGo retourne directement du texte\n",
    "        search_result = ddg_search.invoke(query)\n",
    "        if not search_result or len(search_result.strip()) < 10:\n",
    "            search_result = f\"Aucun resultat pertinent trouve pour: {query}\"\n",
    "    except Exception as e:\n",
    "        search_result = f\"Erreur recherche DuckDuckGo: {str(e)}\"\n",
    "    \n",
    "    new_message = HumanMessage(content=search_result, name=\"web_researcher\")\n",
    "    return Command(update={\"messages\": [new_message]}, goto=\"executor\")\n",
    "\n",
    "# Alias pour compatibilite\n",
    "tavily_search_node = duckduckgo_search_node\n",
    "\n",
    "from prompts import agent_system_prompt\n",
    "\n",
    "chart_generator_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[python_repl_tool],\n",
    "    prompt=agent_system_prompt(\n",
    "        \"You have access to python and the following libraries: os, sys, inspect, matplotlib, plotly, numpy, pandas, statistics, scipy, sklearn. \"\n",
    "        \"Specify the file path to save the chart in the output.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "def chart_generator_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[\"chart_summarizer\"]]:\n",
    "    result = chart_generator_agent.invoke(state)\n",
    "    print(f\"Chart generator answer: {result['messages'][-1].content}\")\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    goto = \"chart_summarizer\"\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "chart_summary_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[],\n",
    "    prompt=agent_system_prompt(\n",
    "        \"You can only summarize the chart that was generated by the chart generator to answer the user's question. You are working with a researcher colleague and a chart generator colleague. \"\n",
    "        + \"Your task is to generate a standalone, concise summary for the provided chart image saved at a local PATH, where the PATH should be and only be provided by your chart generator colleague. The summary should be no more than 3 sentences and should not mention the chart itself.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "def chart_summary_node(\n",
    "    state: State,\n",
    ") -> Command[Literal[END]]:\n",
    "    result = chart_summary_agent.invoke(state)\n",
    "    print(f\"Chart summarizer answer: {result['messages'][-1].content}\")\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_summarizer\"\n",
    "    )\n",
    "    goto = END\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": result[\"messages\"],\n",
    "            \"final_answer\": result[\"messages\"][-1].content,\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "def synthesizer_node(state: State) -> Command[Literal[END]]:\n",
    "    relevant_msgs = []\n",
    "    for m in state.get(\"messages\", []):\n",
    "        if getattr(m, \"name\", None) in (\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"chart_summarizer\"):\n",
    "            raw_content = m.content\n",
    "            if isinstance(raw_content, list):\n",
    "                text_content = \" \".join([str(item) for item in raw_content])\n",
    "            else:\n",
    "                text_content = str(raw_content) if raw_content else \"\"\n",
    "\n",
    "            if len(text_content) > 15000:\n",
    "                text_content = text_content[:15000] + \"... [TRUNCATED DUE TO LENGTH]\"\n",
    "\n",
    "            relevant_msgs.append(text_content)\n",
    "\n",
    "    messages_list = state.get(\"messages\", [])\n",
    "    if messages_list and hasattr(messages_list[0], \"content\"):\n",
    "        default_query = messages_list[0].content\n",
    "    else:\n",
    "        default_query = \"\"\n",
    "\n",
    "    user_question = state.get(\"user_query\", default_query)\n",
    "\n",
    "    synthesis_instructions = (\n",
    "            \"You are the Synthesizer. Use the context below to directly answer the user's question. \"\n",
    "            \"Perform any lightweight calculations, comparisons, or inferences required. \"\n",
    "            \"Do not invent facts not supported by the context. If data is missing, say what's missing and, if helpful, \"\n",
    "            \"offer a clearly labeled best-effort estimate with assumptions.\\n\\n\"\n",
    "            \"Produce a concise response that fully answers the question, with the following guidance:\\n\"\n",
    "            \"- Start with the direct answer (one short paragraph or a tight bullet list).\\n\"\n",
    "            \"- Include key figures from any 'Results:' tables (e.g., totals, top items).\\n\"\n",
    "            \"- If any message contains citations, include them as a brief 'Citations: [...]' line.\\n\"\n",
    "            \"- Keep the output crisp; avoid meta commentary or tool instructions.\"\n",
    "        )\n",
    "\n",
    "    summary_prompt = [\n",
    "        HumanMessage(content=(\n",
    "            f\"User question: {user_question}\\n\\n\"\n",
    "            f\"{synthesis_instructions}\\n\\n\"\n",
    "            f\"Context:\\n\\n\" + \"\\n\\n---\\n\\n\".join(relevant_msgs)\n",
    "        ))\n",
    "    ]\n",
    "    llm_reply = llm.invoke(summary_prompt)\n",
    "\n",
    "    reply_content = llm_reply.content\n",
    "    if isinstance(reply_content, list):\n",
    "        reply_text = \"\".join([c if isinstance(c, str) else str(c) for c in reply_content])\n",
    "    else:\n",
    "        reply_text = str(reply_content)\n",
    "    answer = reply_text.strip()\n",
    "    print(f\"Synthesizer answer: {answer}\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"final_answer\": answer,\n",
    "            \"messages\": [HumanMessage(content=answer, name=\"synthesizer\")],\n",
    "        },\n",
    "        goto=END,\n",
    "    )\n",
    "\n",
    "provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
    "\n",
    "f_groundedness = (Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on({\"source\": Selector(span_type=SpanAttributes.SpanType.RETRIEVAL, span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS, collect_list=True,)})\n",
    "    .on_output())\n",
    "\n",
    "f_answer_relevance = (Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output())\n",
    "\n",
    "f_context_relevance = (Feedback(provider.context_relevance_with_cot_reasons, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on({\"context\": Selector(span_type=SpanAttributes.SpanType.RETRIEVAL, span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS, collect_list=False,)})\n",
    "    .aggregate(np.mean))\n",
    "\n",
    "gpa_eval_provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
    "\n",
    "f_logical_consistency = (Feedback(gpa_eval_provider.logical_consistency_with_cot_reasons, name=\"Logical Consistency\")\n",
    "    .on({\"trace\": Selector(trace_level=True)}))\n",
    "\n",
    "f_execution_efficiency = (Feedback(gpa_eval_provider.execution_efficiency_with_cot_reasons, name=\"Execution Efficiency\")\n",
    "    .on({\"trace\": Selector(trace_level=True)}))\n",
    "\n",
    "f_plan_adherence = (Feedback(gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Adherence\")\n",
    "    .on({\n",
    "            \"prompt\": Selector(span_attribute=\"retrieved_plan\"),\n",
    "            \"response\": Selector(span_attribute=\"retrieved_execution\")\n",
    "        }))\n",
    "\n",
    "f_plan_quality = (\n",
    "    Feedback(gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Quality\")\n",
    "    .on({\n",
    "            \"prompt\": Selector(span_attribute=\"retrieved_query\"),\n",
    "            \"response\": Selector(span_attribute=\"retrieved_plan\")\n",
    "        }))\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "    display(HTML(f'<div style=\"font-size: 15px; word-wrap: break-word; width: {width}px;\">{html_text}</div>'))\n",
    "\n",
    "def display_eval_reason(text, width=800):    html_text = cleaned_text.replace('\\n', '<br><br>')\n",
    "\n",
    "    raw_text = str(text).rstrip()    cleaned_text = re.sub(r\"\\s*Score:\\s*-?\\d+(?:\\.\\d+)?\\s*$\", \"\", raw_text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2X76Ix3smig"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "load_dotenv(override=True)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQnIEtuSrSej"
   },
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT0DTuaYrSek"
   },
   "source": [
    "## 6.1 Add inline evaluations (skipped, already set in helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCfJPCIyrSel"
   },
   "source": [
    "## 6.2 Update the planning prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAUWHUW8rSel"
   },
   "source": [
    "Add pre-conditions, post-conditions, and goals to each step in the agent's plan.\n",
    "\n",
    "Adding this explicit detail helps the executor understand the goal of each step, which improves tool calling and agent decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfxnL3yJrSem"
   },
   "outputs": [],
   "source": [
    "#import helper\n",
    "#import prompts\n",
    "#from langchain.schema import HumanMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "RECURSION_LIMIT = 15\n",
    "\n",
    "original_plan_prompt_fn = plan_prompt\n",
    "\n",
    "def patched_plan_prompt(state):\n",
    "    # FIX: Call the saved original function, NOT the global 'plan_prompt'\n",
    "    base = original_plan_prompt_fn(state).content\n",
    "    insertion = '\"action\": \"string\",\\n            \"pre_conditions\": [\"string\", ...],\\n            \"post_conditions\": [\"string\", ...],\\n            \"goal\": \"string\",'\n",
    "    base = base.replace('\"action\": \"string\",', insertion)\n",
    "\n",
    "    current_step = state.get(\"current_step\", 1)\n",
    "    used = max(0, int(current_step) - 1)\n",
    "    remaining = max(0, RECURSION_LIMIT - used)\n",
    "    base += (f\"\\n\\n<budget> Actions Budget Used: {used}, Max Budget Remaining: {remaining}.  ## IMPORTANT: Make the best use of the available resources. </budget>\")\n",
    "\n",
    "    return HumanMessage(content=base)\n",
    "\n",
    "plan_prompt = patched_plan_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSd6pZTWrSem"
   },
   "source": [
    "## 6.3 Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUqYw7ehrSem"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "#from helper import State, planner_node, executor_node, chart_node, chart_summary_node, synthesizer_node, web_research_node, cortex_agents_research_node\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"web_researcher\", web_research_node)\n",
    "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Preconfigure recursion_limit once (avoid passing it on every invoke).\n",
    "try: graph = graph.with_config({\"recursion_limit\": RECURSION_LIMIT})\n",
    "except Exception: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKWpOyHKrSem"
   },
   "source": [
    "## 6.4 Create a TruLens session for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "me21sjYtrSem",
    "outputId": "8b1ed10c-7080-4509-fc0f-d67b54542645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens.core.session import TruSession\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "\n",
    "# Initialize connector with SQLite database one folder back\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
    "\n",
    "# Create TruSession with the custom connector\n",
    "session = TruSession(connector=connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLu2tCuErSen"
   },
   "source": [
    "## 6.5 Register the new version of the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t2-vzKprSen"
   },
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    <p>ðŸš¨ &nbsp; In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5NKZo3wOiKd",
    "outputId": "6e77eef7-4996-446f-f1d0-0c1fa744b419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting astream_events\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting astream_events\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting stream_mode\n",
      "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting astream_events\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting astream_events\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting stream_mode\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.langgraph import TruGraph\n",
    "from trulens.core.schema.feedback import FeedbackMode\n",
    "\n",
    "#from helper import f_answer_relevance, f_context_relevance, f_groundedness, f_logical_consistency, f_execution_efficiency, f_plan_adherence, f_plan_quality\n",
    "\n",
    "selected_feedbacks = [f_answer_relevance, f_context_relevance, f_groundedness, f_logical_consistency, f_execution_efficiency, f_plan_adherence, f_plan_quality]\n",
    "\n",
    "tru_recorder = TruGraph(\n",
    "    graph,\n",
    "    app_name=\"Research Data Agent\",\n",
    "    app_version=\"L6: Inline evals + sub-goals in planning prompt\",\n",
    "    feedbacks=selected_feedbacks,\n",
    "    feedback_mode=FeedbackMode.WITH_APP_THREAD,\n",
    "    selector_check_warning=True\n",
    "    # selector_nocheck=True # selector_check_warning=False, # selector_nocheck=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jr5SfyxVpyzI",
    "outputId": "a3fd0739-e1d2-41ce-ed94-f018b46f8cc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [opentelemetry.instrumentation.instrumentor] Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "#@title Faire un seul TracerProvider global + export Phoenix + instrumentation LangChain ---\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider as SDKTracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "tp = trace.get_tracer_provider()\n",
    "if not isinstance(tp, SDKTracerProvider):\n",
    "    print(\"âš ï¸ TracerProvider OTEL inattendu. Assurez-vous que TRULENS_OTEL_TRACING=1 et que TruGraph est initialisÃ© avant ce bloc.\")\n",
    "\n",
    "# Ajouter un exporter Phoenix (OTLP HTTP) AU provider global (au lieu de laisser Phoenix/TruLens se battre)\n",
    "_exporter = None\n",
    "try:\n",
    "    from phoenix.otel import HTTPSpanExporter  # type: ignore\n",
    "    _exporter = HTTPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
    "except Exception:\n",
    "    try:\n",
    "        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter  # type: ignore\n",
    "        _exporter = OTLPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Phoenix exporter non initialisÃ©: {e}\")\n",
    "\n",
    "if _exporter is not None:\n",
    "    try:\n",
    "        tp.add_span_processor(BatchSpanProcessor(_exporter))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Impossible d'ajouter le span processor Phoenix: {e}\")\n",
    "\n",
    "# Instrumentation OpenInference (spans LLM/tools) branchÃ©e sur le provider global TruLens\n",
    "try:\n",
    "    LangChainInstrumentor().instrument(tracer_provider=tp)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Instrumentation LangChain dÃ©jÃ  active ou erreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3st18lorSen"
   },
   "source": [
    "## 6.6 Re-test the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7InGlbkorSen"
   },
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    <p>ðŸš¨ &nbsp;<b>Run Results:</b> In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLTWHO_TROs8"
   },
   "source": [
    "**Query 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "hDP1BTjyOoug",
    "outputId": "abac5b2d-17e6-4ce2-9b3a-68a792fca1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the top 5 largest cities in France by population ? Chart the population value for each.\n",
      "\u001b[1m[tasks]\u001b[0m {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}\n",
      "\u001b[1m[debug]\u001b[0m {'step': 1, 'timestamp': '2026-01-08T07:33:04.885667+00:00', 'type': 'task', 'payload': {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR [trulens.core.otel.instrument] Error setting attributes: 'NoneType' object has no attribute 'update'\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1580204302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;34m\"enabled_agents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cortex_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"web_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_generator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_summarizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"synthesizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             }\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tasks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36minstrumented_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# Handle sync generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0minstrumented_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m                         \u001b[0;31m# Each chunk typically contains node updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2644\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36mfiltered_wrapper\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minstrumented_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;31m# Apply the wrapper using wrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2330382181.py\u001b[0m in \u001b[0;36mplanner_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# 1. Invoke LLM with the planner prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mllm_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreasoning_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplan_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# 2. Validate JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         if (\n\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                     raw_response = (\n\u001b[0;32m-> 1354\u001b[0;31m                         self.root_client.chat.completions.with_raw_response.parse(\n\u001b[0m\u001b[1;32m   1355\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    query = \"What are the top 5 largest cities in France by population ? Chart the population value for each.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAkVRuo_rSen"
   },
   "outputs": [],
   "source": [
    "records, feedback = session.get_records_and_feedback()\n",
    "if not records.empty:\n",
    "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
    "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
    "else:\n",
    "    print(\"âŒ No records found. Check for errors in the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCMGL6KerSen"
   },
   "source": [
    "**Query 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehcg__GcRGAV"
   },
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc3-VIqQrSeo"
   },
   "outputs": [],
   "source": [
    "records, feedback = session.get_records_and_feedback()\n",
    "if not records.empty:\n",
    "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
    "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
    "else:\n",
    "    print(\"âŒ No records found. Check for errors in the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6faL_OQVrSeo"
   },
   "source": [
    "**Query 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_idPk2cRYd3"
   },
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    query = \"Identify the largest laboratories studying and developping LLM, then find major topics of those companies in 2026, and find news article about top topics.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
    "\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1M-xIWy0rSeo"
   },
   "outputs": [],
   "source": [
    "records, feedback = session.get_records_and_feedback()\n",
    "if not records.empty:\n",
    "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
    "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
    "else:\n",
    "    print(\"âŒ No records found. Check for errors in the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYNQla7zrSeo"
   },
   "source": [
    "## 6.7 Launch TruLens dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sydoRIBerSeo"
   },
   "source": [
    "By comparing to the previous version, we can validate the changes.\n",
    "\n",
    "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkdZ9D11w1E7"
   },
   "outputs": [],
   "source": [
    "# @title ðŸš€ Launch Dashboard (Force Port 8502)\n",
    "!pip install -q trulens-dashboard\n",
    "from google.colab import output\n",
    "from trulens.core import TruSession\n",
    "import time\n",
    "\n",
    "session = TruSession()\n",
    "\n",
    "# Stop any existing dashboards\n",
    "try:\n",
    "    from trulens.dashboard import stop_dashboard\n",
    "    stop_dashboard(force=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"â³ Starting Dashboard on port 8502...\")\n",
    "session.start_dashboard(port=8502, force=True)\n",
    "time.sleep(5) # Give it time to spin up\n",
    "\n",
    "print(\"âœ… Dashboard ready.\")\n",
    "#output.serve_kernel_port_as_iframe(8502, height=1000)\n",
    "output.serve_kernel_port_as_window(8502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfjuz6uMxHsL"
   },
   "outputs": [],
   "source": [
    "# @title Alternative Analysis ðŸ“Š > View Leaderboard as DataFrame\n",
    "from trulens.core import TruSession\n",
    "import pandas as pd\n",
    "\n",
    "max_records = 10\n",
    "session = TruSession()\n",
    "\n",
    "# Get the leaderboard (aggregates metrics by App ID)\n",
    "print(\"ðŸ“Š Leaderboard:\")\n",
    "display(session.get_leaderboard())\n",
    "\n",
    "# OPTIONAL: Get all raw records to debug specific failures\n",
    "print(f\"\\nðŸ“ Last {max_records} Raw Records:\")\n",
    "records, feedback = session.get_records_and_feedback()\n",
    "if not records.empty:\n",
    "    # Show relevant columns only\n",
    "    cols = ['input', 'output', 'latency', 'total_cost'] + [c for c in records.columns if 'Groundedness' in c or 'Relevance' in c]\n",
    "    # Filter columns that actually exist\n",
    "    valid_cols = [c for c in cols if c in records.columns]\n",
    "    display(records[valid_cols].tail(max_records))\n",
    "else:\n",
    "    print(\"No records found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW4QHDdZrSep"
   },
   "source": [
    "**What other improvements could be also done?**\n",
    "- In this course, we focused on evaluating the end-to-end agent behavior. We could have also tested the behavior of each specialized agent separately to optimize their prompt and design.\n",
    "- We could have added other metrics for inline-evaluations.\n",
    "- We could also updated the prompt of the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xqf_odiwc73"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwNMoBWeuKr_"
   },
   "source": [
    "# Ajout des modules RAG et *SQL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbs1wgUQuQ-g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHYjF8tuuZkA"
   },
   "source": [
    "# Vers l'optimisation\n",
    "\n",
    "> TODO: vÃ©rifications en cours: https://chatgpt.com/c/69583b8d-16b4-8327-9cc0-3b5baff84b01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS9dJ-dku1UE"
   },
   "outputs": [],
   "source": [
    "#@title Instalation de l'optimiseur gÃ©nÃ©ratif Trace avec un mÃ©canisme de log externe\n",
    "!pip install \"git+https://github.com/doxav/NewTrace.git@json-logs-and-traces-IO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBxC-S3-vkVD"
   },
   "source": [
    "RÃ©alisez dans la cellule ci-dessous deux exemples d'optimisations avec Trace avec succÃ¨s depuis un exemple existant:\n",
    "- https://github.com/AgentOpt/OpenTrace/tree/main/examples\n",
    "- https://agentopt.github.io/OpenTrace/#code-examples (attention la doc a Ã©tÃ© gÃ©nÃ©rÃ©e par IA gÃ©nÃ©rative, il peut y avoir des incohÃ©rences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-KFEvjYweCz"
   },
   "outputs": [],
   "source": [
    "# Exemple 1: Optimisation simple de prompt avec Trace\n",
    "# Cet exemple montre comment optimiser un prompt simple avec feedback\n",
    "\n",
    "from opto.trace import node, GRAPH\n",
    "from opto.optimizers import OptoPrime\n",
    "\n",
    "@node\n",
    "def generate_answer(question: str, context: str, template: str) -> str:\n",
    "    \"\"\"Generate an answer using a prompt template.\"\"\"\n",
    "    prompt = template.format(question=question, context=context)\n",
    "    # Simulation d'un LLM call\n",
    "    return f\"Based on {context}, the answer to '{question}' is...\"\n",
    "\n",
    "# Template initial\n",
    "initial_template = \"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "# Execution\n",
    "GRAPH.clear()\n",
    "result = generate_answer(\n",
    "    question=\"What is RAG?\",\n",
    "    context=\"RAG combines retrieval and generation\",\n",
    "    template=initial_template\n",
    ")\n",
    "\n",
    "# Feedback simulÃ©\n",
    "feedback = \"The answer should be more detailed and cite the context explicitly.\"\n",
    "\n",
    "# Optimisation\n",
    "optimizer = OptoPrime()\n",
    "optimized_params = optimizer.optimize(\n",
    "    trace=GRAPH,\n",
    "    feedback=feedback,\n",
    "    max_iterations=2\n",
    ")\n",
    "\n",
    "print(\"Template initial:\", initial_template)\n",
    "print(\"Template optimise:\", optimized_params.get('template', initial_template))\n",
    "print(\"\\nCet exemple montre l'optimisation de prompt avec Trace/OptoPrime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29eOwLnQw7Ad"
   },
   "outputs": [],
   "source": [
    "# Exemple 2: Optimisation de parametres RAG avec Trace\n",
    "# Montre comment optimiser des hyperparametres (k, chunk_size) avec feedback\n",
    "\n",
    "from opto.trace import node, GRAPH\n",
    "from opto.optimizers import OptoPrime\n",
    "\n",
    "@node\n",
    "def retrieve_documents(query: str, k: int, chunk_size: int) -> list:\n",
    "    \"\"\"Retrieve documents with configurable parameters.\"\"\"\n",
    "    # Simulation de retrieval\n",
    "    return [f\"Doc{i} (chunk_size={chunk_size})\" for i in range(k)]\n",
    "\n",
    "@node  \n",
    "def evaluate_retrieval(docs: list, relevance_threshold: float) -> dict:\n",
    "    \"\"\"Evaluate retrieval quality.\"\"\"\n",
    "    # Simulation d'evaluation\n",
    "    relevance = 0.7 if len(docs) >= 3 else 0.5\n",
    "    return {\n",
    "        'score': relevance,\n",
    "        'feedback': 'Good' if relevance > relevance_threshold else 'Need more context'\n",
    "    }\n",
    "\n",
    "# Configuration initiale\n",
    "initial_k = 3\n",
    "initial_chunk_size = 500\n",
    "\n",
    "# Execution\n",
    "GRAPH.clear()\n",
    "docs = retrieve_documents(query=\"What is multi-agent RL?\", k=initial_k, chunk_size=initial_chunk_size)\n",
    "eval_result = evaluate_retrieval(docs, relevance_threshold=0.6)\n",
    "\n",
    "print(f\"Config initiale: k={initial_k}, chunk_size={initial_chunk_size}\")\n",
    "print(f\"Score: {eval_result['score']}, Feedback: {eval_result['feedback']}\")\n",
    "\n",
    "# Optimisation\n",
    "optimizer = OptoPrime()\n",
    "optimized = optimizer.optimize(\n",
    "    trace=GRAPH,\n",
    "    feedback=\"Increase k to get more context and adjust chunk_size for better granularity\",\n",
    "    max_iterations=2\n",
    ")\n",
    "\n",
    "# Resultats optimises\n",
    "optimized_k = optimized.get('k', initial_k)\n",
    "optimized_chunk_size = optimized.get('chunk_size', initial_chunk_size)\n",
    "\n",
    "print(f\"\\nConfig optimisee: k={optimized_k}, chunk_size={optimized_chunk_size}\")\n",
    "print(\"\\nCet exemple montre l'optimisation de parametres RAG avec Trace/OptoPrime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RucotjFCucbE"
   },
   "outputs": [],
   "source": [
    "# @title CrÃ©ation de trace_optimize_runtime.py (Attention le code de cette section du TP va Ã©voluer)\n",
    "%%writefile trace_optimize_runtime.py\n",
    "\"\"\"\n",
    "trace_optimize_runtime.py\n",
    "\n",
    "Pont minimal et **non-intrusif** entre :\n",
    "\n",
    "- des exÃ©cutions LangGraph instrumentÃ©es par TruLens (au format *records JSON* TruLens et/ou spans OpenTelemetry),\n",
    "- des feedbacks TruLens (RAG triad + GPA, ou toute autre mÃ©trique),\n",
    "- et l'optimiseur de la lib Trace/OptoPrime (fichiers `JSON_OTEL_trace_optim_demo_*.py`).\n",
    "\n",
    "Objectif : permettre une boucle \"run â†’ trace â†’ feedback â†’ optimise â†’ patch\" **sans modifier**\n",
    "le code du graphe LangGraph (nÅ“uds/agents) dÃ©jÃ  existant.\n",
    "\n",
    "Principes clÃ©s\n",
    "-------------\n",
    "1) **PrÃ©server le graphe causal** : on ne \"aplatit\" pas la trace. Les paramÃ¨tres `param.*`\n",
    "   sont attachÃ©s aux spans qui reprÃ©sentent *rÃ©ellement* les Ã©tapes (planner/executor/â€¦),\n",
    "   et une span `evaluator` est ajoutÃ©e uniquement pour porter `eval.*`.\n",
    "2) **CompatibilitÃ© double** :\n",
    "   - si vous avez une trace OTEL/OTLP (ex: TruLens OTEL activÃ©), on l'utilise directement ;\n",
    "   - sinon, on reconstruit une trace OTLP minimale depuis un *Record* TruLens (JSON standard).\n",
    "3) **Optimisation de code** (pas seulement du prompt tuning) :\n",
    "   on expose du code comme paramÃ¨tre trainable via `param.__code_<key>` et on applique\n",
    "   les patches via compilation + hotpatch (in-place si possible, ou remplacement symbolique).\n",
    "\n",
    "Cette implÃ©mentation vise une approche gÃ©nÃ©rique :\n",
    "- pas de fonctions nommÃ©es \"for_l6\" ;\n",
    "- tout est pilotÃ© par des *configurations* (matchers, specs, targets).\n",
    "\n",
    "PrÃ©-requis au runtime\n",
    "---------------------\n",
    "- TruLens : utilisÃ© pour capturer les records et produire les feedbacks.\n",
    "- (Optionnel) OpenTelemetry : si TruLens exporte des spans OTEL, on peut les \"flusher\".\n",
    "- Trace/Opto (repo Trace/opto) : utilisÃ© pour `otlp_traces_to_trace_json`, `ingest_tgj`,\n",
    "  et l'optimiseur OptoPrimeV2.\n",
    "\n",
    "Remarque : ce fichier ne dÃ©pend pas de LangGraph ni TruLens Ã  l'import.\n",
    "Il se contente de manipuler des JSON/dicts, et d'appliquer des patches Python.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import dataclasses\n",
    "import datetime as _dt\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "import time\n",
    "import types\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    MutableMapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Types simples\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "JSONDict = Dict[str, Any]\n",
    "SpanDict = Dict[str, Any]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utilitaires JSON / texte\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def safe_json_dumps(obj: Any, *, max_len: int = 4000) -> str:\n",
    "    \"\"\"\n",
    "    SÃ©rialise `obj` en JSON de maniÃ¨re robuste (fallback str), puis tronque.\n",
    "\n",
    "    Args:\n",
    "        obj: objet Ã  sÃ©rialiser.\n",
    "        max_len: longueur max en caractÃ¨res (au-delÃ , on tronque).\n",
    "\n",
    "    Returns:\n",
    "        str JSON (ou string fallback), tronquÃ©e si nÃ©cessaire.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = json.dumps(obj, ensure_ascii=False, default=str)\n",
    "    except Exception:\n",
    "        s = str(obj)\n",
    "    if max_len and len(s) > max_len:\n",
    "        return s[: max_len - 3] + \"...\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise lÃ©gÃ¨rement un texte (espaces, lignes vides) pour stabiliser des diffs.\n",
    "\n",
    "    Args:\n",
    "        s: texte.\n",
    "\n",
    "    Returns:\n",
    "        texte normalisÃ©.\n",
    "    \"\"\"\n",
    "    s2 = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Ã©vite de dÃ©truire la mise en forme: on enlÃ¨ve juste les trailing spaces\n",
    "    s2 = \"\\n\".join(line.rstrip() for line in s2.splitlines())\n",
    "    return s2.strip() + (\"\\n\" if s2.endswith(\"\\n\") else \"\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# OTLP helpers (structure JSON)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _otlp_attr_value(value: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Encode une valeur Python en valeur OTLP JSON (stringValue/doubleValue/intValue/boolValue).\n",
    "\n",
    "    Note:\n",
    "        Pour rester simple et compatible, on privilÃ©gie stringValue.\n",
    "        Les nombres sont encodÃ©s en doubleValue si possibles.\n",
    "\n",
    "    Returns:\n",
    "        dict au format OTLP \"AnyValue\".\n",
    "    \"\"\"\n",
    "    if isinstance(value, bool):\n",
    "        return {\"boolValue\": bool(value)}\n",
    "    if isinstance(value, int) and not isinstance(value, bool):\n",
    "        # OTLP accepte intValue sous forme de chaÃ®ne ou int selon l'impl; on met int.\n",
    "        return {\"intValue\": int(value)}\n",
    "    if isinstance(value, float):\n",
    "        return {\"doubleValue\": float(value)}\n",
    "    # fallback string\n",
    "    return {\"stringValue\": str(value)}\n",
    "\n",
    "\n",
    "def _otlp_kv(key: str, value: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Construit un attribut OTLP (key/value).\"\"\"\n",
    "    return {\"key\": key, \"value\": _otlp_attr_value(value)}\n",
    "\n",
    "\n",
    "def otlp_is_payload(obj: Any) -> bool:\n",
    "    \"\"\"\n",
    "    DÃ©tecte si `obj` ressemble Ã  un payload OTLP traces JSON.\n",
    "\n",
    "    Args:\n",
    "        obj: objet quelconque.\n",
    "\n",
    "    Returns:\n",
    "        True si la structure contient `resourceSpans`.\n",
    "    \"\"\"\n",
    "    return isinstance(obj, dict) and \"resourceSpans\" in obj\n",
    "\n",
    "\n",
    "def otlp_iter_spans(otlp: JSONDict) -> Iterator[SpanDict]:\n",
    "    \"\"\"\n",
    "    ItÃ¨re sur tous les spans d'un payload OTLP.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP (dict).\n",
    "\n",
    "    Yields:\n",
    "        chaque span (dict) *mutable*.\n",
    "    \"\"\"\n",
    "    for rs in otlp.get(\"resourceSpans\", []) or []:\n",
    "        for ss in rs.get(\"scopeSpans\", []) or []:\n",
    "            for sp in ss.get(\"spans\", []) or []:\n",
    "                yield sp\n",
    "\n",
    "\n",
    "def otlp_span_attrs_to_dict(span: SpanDict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convertit la liste `span[\"attributes\"]` en dict {key: python_value}.\n",
    "\n",
    "    Args:\n",
    "        span: dict OTLP span.\n",
    "\n",
    "    Returns:\n",
    "        dict (valeurs simplifiÃ©es).\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "    for kv in span.get(\"attributes\", []) or []:\n",
    "        k = kv.get(\"key\")\n",
    "        v = kv.get(\"value\", {})\n",
    "        if not k:\n",
    "            continue\n",
    "        # choisir un champ OTLP\n",
    "        if \"stringValue\" in v:\n",
    "            out[k] = v[\"stringValue\"]\n",
    "        elif \"doubleValue\" in v:\n",
    "            out[k] = float(v[\"doubleValue\"])\n",
    "        elif \"intValue\" in v:\n",
    "            out[k] = int(v[\"intValue\"])\n",
    "        elif \"boolValue\" in v:\n",
    "            out[k] = bool(v[\"boolValue\"])\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def otlp_set_span_attribute(span: SpanDict, key: str, value: Any) -> None:\n",
    "    \"\"\"\n",
    "    Ajoute ou remplace un attribut OTLP sur un span.\n",
    "\n",
    "    Args:\n",
    "        span: span OTLP mutable.\n",
    "        key: clÃ© d'attribut.\n",
    "        value: valeur (sera encodÃ©e).\n",
    "    \"\"\"\n",
    "    attrs = span.get(\"attributes\")\n",
    "    if attrs is None:\n",
    "        attrs = []\n",
    "        span[\"attributes\"] = attrs\n",
    "\n",
    "    # replace if exists\n",
    "    for kv in attrs:\n",
    "        if kv.get(\"key\") == key:\n",
    "            kv[\"value\"] = _otlp_attr_value(value)\n",
    "            return\n",
    "\n",
    "    attrs.append(_otlp_kv(key, value))\n",
    "\n",
    "\n",
    "def otlp_get_trace_id(otlp: JSONDict) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Renvoie un traceId (hex) du payload OTLP si prÃ©sent.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP.\n",
    "\n",
    "    Returns:\n",
    "        traceId (32 hex chars) ou None.\n",
    "    \"\"\"\n",
    "    for sp in otlp_iter_spans(otlp):\n",
    "        tid = sp.get(\"traceId\")\n",
    "        if tid:\n",
    "            return tid\n",
    "    return None\n",
    "\n",
    "\n",
    "def _new_trace_id_hex() -> str:\n",
    "    \"\"\"GÃ©nÃ¨re un traceId OTLP (32 hex chars).\"\"\"\n",
    "    return uuid.uuid4().hex  # 32 hex\n",
    "\n",
    "\n",
    "def _new_span_id_hex() -> str:\n",
    "    \"\"\"GÃ©nÃ¨re un spanId OTLP (16 hex chars).\"\"\"\n",
    "    return f\"{random.getrandbits(64):016x}\"\n",
    "\n",
    "\n",
    "def ensure_otlp_shell(\n",
    "    *,\n",
    "    service_name: str = \"app\",\n",
    "    scope_name: str = \"trace_opt\",\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Construit un \"shell\" OTLP vide compatible avec `otlp_traces_to_trace_json`.\n",
    "\n",
    "    Args:\n",
    "        service_name: nom de ressource OTEL.\n",
    "        scope_name: nom du scope.\n",
    "\n",
    "    Returns:\n",
    "        dict OTLP avec `resourceSpans/scopeSpans/spans`.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"resourceSpans\": [\n",
    "            {\n",
    "                \"resource\": {\n",
    "                    \"attributes\": [\n",
    "                        _otlp_kv(\"service.name\", service_name),\n",
    "                    ]\n",
    "                },\n",
    "                \"scopeSpans\": [\n",
    "                    {\n",
    "                        \"scope\": {\"name\": scope_name, \"version\": \"\"},\n",
    "                        \"spans\": [],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def otlp_append_span(otlp: JSONDict, span: SpanDict) -> None:\n",
    "    \"\"\"\n",
    "    Ajoute un span Ã  la premiÃ¨re scopeSpan du payload.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP.\n",
    "        span: span dict.\n",
    "    \"\"\"\n",
    "    rs_list = otlp.setdefault(\"resourceSpans\", [])\n",
    "    if not rs_list:\n",
    "        otlp.update(ensure_otlp_shell())\n",
    "        rs_list = otlp[\"resourceSpans\"]\n",
    "    rs0 = rs_list[0]\n",
    "    ss_list = rs0.setdefault(\"scopeSpans\", [])\n",
    "    if not ss_list:\n",
    "        ss_list.append({\"scope\": {\"name\": \"trace_opt\", \"version\": \"\"}, \"spans\": []})\n",
    "    ss0 = ss_list[0]\n",
    "    spans = ss0.setdefault(\"spans\", [])\n",
    "    spans.append(span)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Capture OTEL -> OTLP (optionnel)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def try_attach_inmemory_span_exporter() -> Tuple[Optional[Any], Optional[Any], str]:\n",
    "    \"\"\"\n",
    "    Tente d'attacher un `InMemorySpanExporter` au TracerProvider global OpenTelemetry.\n",
    "\n",
    "    Pourquoi:\n",
    "        TruLens peut exporter des spans OTEL (OpenTelemetry). Si on peut accrocher un\n",
    "        exporter en mÃ©moire, on peut rÃ©cupÃ©rer la trace OTLP **sans** modifier le graphe.\n",
    "\n",
    "    Returns:\n",
    "        (exporter, processor, status)\n",
    "\n",
    "        - exporter: instance InMemorySpanExporter ou None\n",
    "        - processor: SimpleSpanProcessor ou None\n",
    "        - status: message (ok / warning / error)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from opentelemetry import trace as otel_trace  # type: ignore\n",
    "        from opentelemetry.sdk.trace.export import InMemorySpanExporter, SimpleSpanProcessor  # type: ignore\n",
    "    except Exception as e:\n",
    "        return None, None, f\"OpenTelemetry SDK indisponible: {e}\"\n",
    "\n",
    "    provider = otel_trace.get_tracer_provider()\n",
    "    if not hasattr(provider, \"add_span_processor\"):\n",
    "        return None, None, \"TracerProvider global n'a pas add_span_processor (provider non-SDK ?)\"\n",
    "\n",
    "    try:\n",
    "        exporter = InMemorySpanExporter()\n",
    "        processor = SimpleSpanProcessor(exporter)\n",
    "        provider.add_span_processor(processor)  # type: ignore[attr-defined]\n",
    "        return exporter, processor, \"ok\"\n",
    "    except Exception as e:\n",
    "        return None, None, f\"Erreur lors de l'attachement de l'exporter: {e}\"\n",
    "\n",
    "\n",
    "def flush_inmemory_exporter_to_otlp(\n",
    "    exporter: Any,\n",
    "    *,\n",
    "    service_name: str = \"app\",\n",
    "    scope_name: str = \"inmemory\",\n",
    "    clear: bool = True,\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Convertit les spans collectÃ©s par `InMemorySpanExporter` en payload OTLP JSON.\n",
    "\n",
    "    Args:\n",
    "        exporter: instance InMemorySpanExporter.\n",
    "        service_name: resource.service.name.\n",
    "        scope_name: scopeSpans.scope.name.\n",
    "        clear: si True, vider l'exporter aprÃ¨s lecture.\n",
    "\n",
    "    Returns:\n",
    "        OTLP payload dict.\n",
    "    \"\"\"\n",
    "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
    "\n",
    "    spans = list(getattr(exporter, \"get_finished_spans\")() or [])\n",
    "    if clear and hasattr(exporter, \"clear\"):\n",
    "        exporter.clear()\n",
    "\n",
    "    for sp in spans:\n",
    "        try:\n",
    "            ctx = sp.get_span_context()\n",
    "            trace_id = f\"{ctx.trace_id:032x}\"\n",
    "            span_id = f\"{ctx.span_id:016x}\"\n",
    "        except Exception:\n",
    "            # fallback (rare)\n",
    "            trace_id = _new_trace_id_hex()\n",
    "            span_id = _new_span_id_hex()\n",
    "\n",
    "        parent_span_id = \"\"\n",
    "        try:\n",
    "            parent = getattr(sp, \"parent\", None)\n",
    "            if parent is not None:\n",
    "                parent_span_id = f\"{parent.span_id:016x}\"\n",
    "        except Exception:\n",
    "            parent_span_id = \"\"\n",
    "\n",
    "        name = getattr(sp, \"name\", \"span\")\n",
    "        start_ns = int(getattr(sp, \"start_time\", time.time_ns()))\n",
    "        end_ns = int(getattr(sp, \"end_time\", start_ns + 1_000_000))\n",
    "\n",
    "        attrs_list: List[Dict[str, Any]] = []\n",
    "        attrs = getattr(sp, \"attributes\", {}) or {}\n",
    "        if isinstance(attrs, dict):\n",
    "            for k, v in attrs.items():\n",
    "                # Pour rester robuste, on encode en string (Trace/otel_adapter sait parser stringValue).\n",
    "                attrs_list.append(_otlp_kv(str(k), safe_json_dumps(v, max_len=8000)))\n",
    "\n",
    "        otlp_append_span(\n",
    "            otlp,\n",
    "            {\n",
    "                \"traceId\": trace_id,\n",
    "                \"spanId\": span_id,\n",
    "                \"parentSpanId\": parent_span_id,\n",
    "                \"name\": str(name),\n",
    "                \"kind\": \"INTERNAL\",\n",
    "                \"startTimeUnixNano\": start_ns,\n",
    "                \"endTimeUnixNano\": end_ns,\n",
    "                \"attributes\": attrs_list,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return otlp\n",
    "\n",
    "\n",
    "# TruLens record JSON -> OTLP (fallback si pas de spans OTEL disponibles)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def trulens_is_record(obj: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristique: dÃ©tecte si `obj` ressemble Ã  un Record TruLens (JSON standard).\n",
    "\n",
    "    Un Record TruLens (voir doc) contient typiquement `record_id` et `calls`.\n",
    "\n",
    "    Args:\n",
    "        obj: objet.\n",
    "\n",
    "    Returns:\n",
    "        True si on dÃ©tecte des champs \"record_id\" ou \"calls\".\n",
    "    \"\"\"\n",
    "    return isinstance(obj, dict) and (\"calls\" in obj or \"record_id\" in obj or \"main_input\" in obj)\n",
    "\n",
    "\n",
    "def _parse_dt_to_ns(value: Any) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Tente de parser des timestamps TruLens (perf.start_time / perf.end_time) vers ns Unix.\n",
    "\n",
    "    Formats acceptÃ©s (best-effort):\n",
    "      - int / float : supposÃ© Ãªtre des secondes (float) ou ns (int trÃ¨s grand).\n",
    "      - str ISO 8601 : ex \"2025-01-02T12:34:56.123Z\"\n",
    "      - datetime.\n",
    "\n",
    "    Returns:\n",
    "        int nanosecondes, ou None si impossible.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(value, int):\n",
    "        # Heuristique: si trÃ¨s grand, c'est dÃ©jÃ  du ns\n",
    "        if value > 10_000_000_000_000:  # > ~1970 + 4h en ns\n",
    "            return value\n",
    "        # sinon secondes\n",
    "        return int(value * 1_000_000_000)\n",
    "\n",
    "    if isinstance(value, float):\n",
    "        return int(value * 1_000_000_000)\n",
    "\n",
    "    if isinstance(value, _dt.datetime):\n",
    "        if value.tzinfo is None:\n",
    "            value = value.replace(tzinfo=_dt.timezone.utc)\n",
    "        return int(value.timestamp() * 1_000_000_000)\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        # Z -> +00:00\n",
    "        if s.endswith(\"Z\"):\n",
    "            s = s[:-1] + \"+00:00\"\n",
    "        try:\n",
    "            dt = _dt.datetime.fromisoformat(s)\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=_dt.timezone.utc)\n",
    "            return int(dt.timestamp() * 1_000_000_000)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _trulens_call_name(call: JSONDict) -> str:\n",
    "    \"\"\"\n",
    "    Produit un nom de span \"lisible\" pour un call TruLens.\n",
    "\n",
    "    TruLens record appelle ces objets `RecordAppCall` avec un champ `stack` contenant\n",
    "    des Ã©lÃ©ments `RecordAppCallMethod` incluant `path` et `method`.\n",
    "\n",
    "    StratÃ©gie:\n",
    "      - si on a un `path`, on prend son dernier segment (souvent proche du nom de nÅ“ud)\n",
    "      - sinon, on prend `method.name`\n",
    "      - sinon fallback \"call\"\n",
    "\n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "    stack = call.get(\"stack\") or []\n",
    "    top = stack[-1] if isinstance(stack, list) and stack else {}\n",
    "    method = (top.get(\"method\") or {}) if isinstance(top, dict) else {}\n",
    "    path = top.get(\"path\") if isinstance(top, dict) else None\n",
    "\n",
    "    # path est souvent un Lens (liste de segments)\n",
    "    last_seg: Optional[str] = None\n",
    "    if isinstance(path, (list, tuple)) and path:\n",
    "        last = path[-1]\n",
    "        if isinstance(last, str):\n",
    "            last_seg = last\n",
    "        else:\n",
    "            last_seg = str(last)\n",
    "    elif isinstance(path, str) and path:\n",
    "        # ex: \"nodes/planner\"\n",
    "        parts = re.split(r\"[\\\\/]+\", path)\n",
    "        last_seg = parts[-1] if parts else path\n",
    "\n",
    "    mname = None\n",
    "    if isinstance(method, dict):\n",
    "        mname = method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\")\n",
    "\n",
    "    if last_seg and last_seg not in {\"__call__\", \"invoke\", \"run\"}:\n",
    "        return str(last_seg)\n",
    "    if mname:\n",
    "        return str(mname)\n",
    "    return \"call\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _CallSpan:\n",
    "    \"\"\"Structure interne pour reconstruire une hiÃ©rarchie approximative.\"\"\"\n",
    "    call_idx: int\n",
    "    name: str\n",
    "    call_id: str\n",
    "    start_ns: Optional[int]\n",
    "    end_ns: Optional[int]\n",
    "    stack_sig: Tuple[str, ...]\n",
    "    args: Any = None\n",
    "    rets: Any = None\n",
    "    error: Optional[str] = None\n",
    "    parent_idx: Optional[int] = None\n",
    "    span_id: str = field(default_factory=_new_span_id_hex)\n",
    "\n",
    "\n",
    "def _call_stack_signature(call: JSONDict) -> Tuple[str, ...]:\n",
    "    \"\"\"\n",
    "    Construit une signature (tuple) Ã  partir de `call.stack` pour aider Ã  infÃ©rer la hiÃ©rarchie.\n",
    "\n",
    "    Returns:\n",
    "        tuple de strings.\n",
    "    \"\"\"\n",
    "    sig: List[str] = []\n",
    "    stack = call.get(\"stack\") or []\n",
    "    if not isinstance(stack, list):\n",
    "        return tuple()\n",
    "\n",
    "    for frame in stack:\n",
    "        if not isinstance(frame, dict):\n",
    "            continue\n",
    "        path = frame.get(\"path\")\n",
    "        method = frame.get(\"method\") or {}\n",
    "        # path normalisÃ©\n",
    "        if isinstance(path, (list, tuple)):\n",
    "            p = \"/\".join(str(x) for x in path)\n",
    "        else:\n",
    "            p = str(path) if path is not None else \"\"\n",
    "        m = \"\"\n",
    "        if isinstance(method, dict):\n",
    "            m = str(method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\") or \"\")\n",
    "        sig.append(f\"{p}::{m}\".strip(\":\"))\n",
    "    return tuple(sig)\n",
    "\n",
    "\n",
    "def trulens_record_to_otlp(\n",
    "    record: JSONDict,\n",
    "    *,\n",
    "    service_name: str = \"trulens\",\n",
    "    scope_name: str = \"trulens_record\",\n",
    "    trace_id: Optional[str] = None,\n",
    "    include_root_span: bool = True,\n",
    "    max_io_chars: int = 4000,\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Convertit un Record TruLens (JSON) en payload OTLP minimal.\n",
    "\n",
    "    Cette conversion est un *fallback* quand vous n'avez pas de spans OTEL disponibles.\n",
    "    Elle reconstruit une hiÃ©rarchie de spans Ã  partir des `perf` timestamps (si prÃ©sents),\n",
    "    sinon Ã  partir de signatures de stack (heuristique).\n",
    "\n",
    "    Args:\n",
    "        record: dict JSON TruLens (record).\n",
    "        service_name: service.name OTEL.\n",
    "        scope_name: scope OTEL.\n",
    "        trace_id: si fourni, utilisÃ© comme traceId.\n",
    "        include_root_span: ajoute un span racine \"record\" (recommandÃ©).\n",
    "        max_io_chars: taille max pour input.value / output.value.\n",
    "\n",
    "    Returns:\n",
    "        OTLP payload dict.\n",
    "    \"\"\"\n",
    "    if not trulens_is_record(record):\n",
    "        raise ValueError(\"L'objet fourni ne ressemble pas Ã  un Record TruLens JSON.\")\n",
    "\n",
    "    trace_id = trace_id or _new_trace_id_hex()\n",
    "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
    "\n",
    "    calls = record.get(\"calls\") or []\n",
    "    if not isinstance(calls, list):\n",
    "        calls = []\n",
    "\n",
    "    spans: List[_CallSpan] = []\n",
    "    for idx, call in enumerate(calls):\n",
    "        if not isinstance(call, dict):\n",
    "            continue\n",
    "        call_id = str(call.get(\"call_id\") or call.get(\"callId\") or f\"call-{idx}\")\n",
    "        name = _trulens_call_name(call)\n",
    "        perf = call.get(\"perf\") or {}\n",
    "        start_ns = _parse_dt_to_ns(perf.get(\"start_time\") or perf.get(\"startTime\") or perf.get(\"start\"))\n",
    "        end_ns = _parse_dt_to_ns(perf.get(\"end_time\") or perf.get(\"endTime\") or perf.get(\"end\"))\n",
    "        stack_sig = _call_stack_signature(call)\n",
    "\n",
    "        spans.append(\n",
    "            _CallSpan(\n",
    "                call_idx=idx,\n",
    "                name=name,\n",
    "                call_id=call_id,\n",
    "                start_ns=start_ns,\n",
    "                end_ns=end_ns,\n",
    "                stack_sig=stack_sig,\n",
    "                args=call.get(\"args\"),\n",
    "                rets=call.get(\"rets\"),\n",
    "                error=call.get(\"error\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Heuristique de hiÃ©rarchie:\n",
    "    # 1) si des timestamps existent pour la majoritÃ©, on utilise l'inclusion d'intervalles\n",
    "    # 2) sinon, on utilise la relation \"stack prefix\" la plus rÃ©cente\n",
    "    have_times = sum(1 for s in spans if s.start_ns is not None and s.end_ns is not None)\n",
    "    use_interval = have_times >= max(2, int(0.6 * len(spans))) if spans else False\n",
    "\n",
    "    if use_interval:\n",
    "        # Ordre: start asc, end desc (pour bien gÃ©rer les enveloppes)\n",
    "        spans_sorted = sorted(\n",
    "            spans,\n",
    "            key=lambda s: (\n",
    "                s.start_ns if s.start_ns is not None else 0,\n",
    "                -(s.end_ns if s.end_ns is not None else 0),\n",
    "            ),\n",
    "        )\n",
    "        stack: List[_CallSpan] = []\n",
    "        for s in spans_sorted:\n",
    "            s_start = s.start_ns if s.start_ns is not None else 0\n",
    "            # pop les spans qui se terminent avant le start courant\n",
    "            while stack and (stack[-1].end_ns is not None) and s_start >= (stack[-1].end_ns or 0):\n",
    "                stack.pop()\n",
    "            if stack:\n",
    "                s.parent_idx = stack[-1].call_idx\n",
    "            stack.append(s)\n",
    "        # spans_sorted contient des objets de la mÃªme liste => parent_idx est appliquÃ©\n",
    "    else:\n",
    "        # stack prefix: on mappe signature -> dernier idx\n",
    "        last_by_sig: Dict[Tuple[str, ...], int] = {}\n",
    "        for s in spans:\n",
    "            parent_sig = s.stack_sig[:-1] if s.stack_sig else tuple()\n",
    "            if parent_sig in last_by_sig:\n",
    "                s.parent_idx = last_by_sig[parent_sig]\n",
    "            # enregistrer ce call comme dernier pour sa signature\n",
    "            last_by_sig[s.stack_sig] = s.call_idx\n",
    "\n",
    "    # Root span optionnel\n",
    "    root_span_id = _new_span_id_hex()\n",
    "    root_end = max((s.end_ns or 0) for s in spans) if spans else time.time_ns()\n",
    "    root_start = min((s.start_ns or root_end) for s in spans) if spans else root_end - 1_000_000\n",
    "\n",
    "    if include_root_span:\n",
    "        root_span: SpanDict = {\n",
    "            \"traceId\": trace_id,\n",
    "            \"spanId\": root_span_id,\n",
    "            \"parentSpanId\": \"\",\n",
    "            \"name\": \"record\",\n",
    "            \"kind\": \"INTERNAL\",\n",
    "            \"startTimeUnixNano\": int(root_start),\n",
    "            \"endTimeUnixNano\": int(root_end),\n",
    "            \"attributes\": [\n",
    "                _otlp_kv(\"trulens.record_id\", record.get(\"record_id\") or record.get(\"recordId\") or \"\"),\n",
    "                _otlp_kv(\"input.value\", safe_json_dumps(record.get(\"main_input\"), max_len=max_io_chars)),\n",
    "                _otlp_kv(\"output.value\", safe_json_dumps(record.get(\"main_output\"), max_len=max_io_chars)),\n",
    "            ],\n",
    "        }\n",
    "        otlp_append_span(otlp, root_span)\n",
    "\n",
    "    # Convert calls to spans\n",
    "    now_ns = time.time_ns()\n",
    "    for s in spans:\n",
    "        start = s.start_ns or (now_ns + s.call_idx * 1_000_000)\n",
    "        end = s.end_ns or (start + 500_000)\n",
    "\n",
    "        parent_span_id = \"\"\n",
    "        if s.parent_idx is not None:\n",
    "            # retrouver le parent span_id\n",
    "            parent = next((p for p in spans if p.call_idx == s.parent_idx), None)\n",
    "            if parent is not None:\n",
    "                parent_span_id = parent.span_id\n",
    "        elif include_root_span:\n",
    "            parent_span_id = root_span_id\n",
    "\n",
    "        span: SpanDict = {\n",
    "            \"traceId\": trace_id,\n",
    "            \"spanId\": s.span_id,\n",
    "            \"parentSpanId\": parent_span_id,\n",
    "            \"name\": s.name,\n",
    "            \"kind\": \"INTERNAL\",\n",
    "            \"startTimeUnixNano\": int(start),\n",
    "            \"endTimeUnixNano\": int(end),\n",
    "            \"attributes\": [\n",
    "                _otlp_kv(\"trulens.call_id\", s.call_id),\n",
    "                _otlp_kv(\"input.value\", safe_json_dumps(s.args, max_len=max_io_chars)),\n",
    "                _otlp_kv(\"output.value\", safe_json_dumps(s.rets, max_len=max_io_chars)),\n",
    "            ],\n",
    "        }\n",
    "        if s.error:\n",
    "            span[\"attributes\"].append(_otlp_kv(\"error.value\", s.error))\n",
    "        otlp_append_span(otlp, span)\n",
    "\n",
    "    return otlp\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SÃ©lection de spans / injection de paramÃ¨tres\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SpanMatcher:\n",
    "    \"\"\"\n",
    "    SÃ©lecteur de spans (OTLP) basÃ© sur des heuristiques simples.\n",
    "\n",
    "    Vous pouvez matcher par :\n",
    "    - substring(s) sur le nom (`name_contains`)\n",
    "    - regex(s) sur le nom (`name_regex`)\n",
    "    - prÃ©sence de certaines clÃ©s d'attributs (`has_attrs`)\n",
    "    - substring(s) sur la valeur d'un attribut (`attr_contains`)\n",
    "\n",
    "    C'est volontairement simple pour rester gÃ©nÃ©rique et portable.\n",
    "    \"\"\"\n",
    "    name_contains: Tuple[str, ...] = ()\n",
    "    name_regex: Tuple[str, ...] = ()\n",
    "    has_attrs: Tuple[str, ...] = ()\n",
    "    attr_contains: Mapping[str, Tuple[str, ...]] = dataclasses.field(default_factory=dict)\n",
    "\n",
    "    def matches(self, span: SpanDict) -> bool:\n",
    "        \"\"\"Retourne True si `span` satisfait ce matcher.\"\"\"\n",
    "        name = str(span.get(\"name\") or \"\")\n",
    "        lname = name.lower()\n",
    "\n",
    "        if self.name_contains:\n",
    "            if not any(sub.lower() in lname for sub in self.name_contains):\n",
    "                return False\n",
    "\n",
    "        if self.name_regex:\n",
    "            ok = False\n",
    "            for pat in self.name_regex:\n",
    "                try:\n",
    "                    if re.search(pat, name):\n",
    "                        ok = True\n",
    "                        break\n",
    "                except re.error:\n",
    "                    continue\n",
    "            if not ok:\n",
    "                return False\n",
    "\n",
    "        if self.has_attrs or self.attr_contains:\n",
    "            attrs = otlp_span_attrs_to_dict(span)\n",
    "            if self.has_attrs:\n",
    "                if not all(k in attrs for k in self.has_attrs):\n",
    "                    return False\n",
    "            for k, subs in self.attr_contains.items():\n",
    "                v = str(attrs.get(k, \"\"))\n",
    "                lv = v.lower()\n",
    "                if not any(s.lower() in lv for s in subs):\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "def select_spans(otlp: JSONDict, matcher: SpanMatcher) -> List[SpanDict]:\n",
    "    \"\"\"\n",
    "    Retourne la liste des spans matching `matcher`.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP.\n",
    "        matcher: SpanMatcher.\n",
    "\n",
    "    Returns:\n",
    "        liste de spans (dicts mutables).\n",
    "    \"\"\"\n",
    "    return [sp for sp in otlp_iter_spans(otlp) if matcher.matches(sp)]\n",
    "\n",
    "\n",
    "def select_one_span(otlp: JSONDict, matcher: SpanMatcher) -> Optional[SpanDict]:\n",
    "    \"\"\"\n",
    "    Retourne le premier span matching (ou None).\n",
    "\n",
    "    Astuce:\n",
    "        Pratique pour choisir le parent d'un span evaluator, etc.\n",
    "\n",
    "    Returns:\n",
    "        span dict ou None.\n",
    "    \"\"\"\n",
    "    for sp in otlp_iter_spans(otlp):\n",
    "        if matcher.matches(sp):\n",
    "            return sp\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SpÃ©cifications de paramÃ¨tres entraÃ®nables\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ParamSpec:\n",
    "    \"\"\"\n",
    "    DÃ©crit un paramÃ¨tre Ã  :\n",
    "      1) exposer dans la trace (OTLP) via `param.<name>`\n",
    "      2) Ã©ventuellement appliquer au runtime lors d'une update.\n",
    "\n",
    "    Attributes:\n",
    "        name: nom logique (ex: \"planner_addendum\" ou \"__code_planner_node\").\n",
    "        get_value: fonction 0-arg retournant la valeur courante (str conseillÃ©).\n",
    "        apply_update: fonction (new_value:str) -> None, appliquant une update.\n",
    "        attach_to: SpanMatcher indiquant sur quel(s) span(s) Ã©crire l'attribut param.*.\n",
    "        trainable: si False, l'optimiseur ne doit pas toucher ce param.\n",
    "        description: description courte injectÃ©e cÃ´tÃ© optimiseur (conseillÃ©e pour code).\n",
    "        normalize: optionnel, transforme la valeur avant injection (ex: normaliser espaces).\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    get_value: Callable[[], Any]\n",
    "    apply_update: Optional[Callable[[str], None]] = None\n",
    "    attach_to: Optional[SpanMatcher] = None\n",
    "    trainable: bool = True\n",
    "    description: str = \"\"\n",
    "    normalize: Optional[Callable[[str], str]] = normalize_whitespace\n",
    "\n",
    "    def value_as_str(self) -> str:\n",
    "        \"\"\"Renvoie la valeur courante en string (avec normalisation si configurÃ©e).\"\"\"\n",
    "        v = self.get_value()\n",
    "        s = v if isinstance(v, str) else safe_json_dumps(v, max_len=8000)\n",
    "        if self.normalize:\n",
    "            try:\n",
    "                s = self.normalize(s)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return s\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Prompt tuning gÃ©nÃ©rique (addendum non-intrusif)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class TextOverrideStore:\n",
    "    \"\"\"\n",
    "    Store simple (en mÃ©moire) pour des overrides textuels.\n",
    "\n",
    "    Usage typique:\n",
    "        store = TextOverrideStore()\n",
    "        store.set(\"planner_addendum\", \"...\")\n",
    "\n",
    "    On peut l'utiliser avec `wrap_prompt_builder_with_addendum` pour modifier\n",
    "    une fonction qui retourne un prompt (str ou BaseMessage LangChain).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._values: Dict[str, str] = {}\n",
    "\n",
    "    def get(self, key: str, default: str = \"\") -> str:\n",
    "        return str(self._values.get(key, default))\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._values[str(key)] = str(value)\n",
    "\n",
    "    def as_param_spec(\n",
    "        self,\n",
    "        *,\n",
    "        name: str,\n",
    "        attach_to: Optional[SpanMatcher],\n",
    "        trainable: bool = True,\n",
    "        description: str = \"\",\n",
    "    ) -> ParamSpec:\n",
    "        \"\"\"\n",
    "        Construit un ParamSpec \"texte\" connectÃ© Ã  ce store.\n",
    "\n",
    "        Args:\n",
    "            name: nom du param (clÃ© dans le store).\n",
    "            attach_to: oÃ¹ accrocher dans la trace.\n",
    "            trainable: bool.\n",
    "            description: aide l'optimiseur.\n",
    "\n",
    "        Returns:\n",
    "            ParamSpec.\n",
    "        \"\"\"\n",
    "        return ParamSpec(\n",
    "            name=name,\n",
    "            get_value=lambda: self.get(name, \"\"),\n",
    "            apply_update=lambda v: self.set(name, v),\n",
    "            attach_to=attach_to,\n",
    "            trainable=trainable,\n",
    "            description=description,\n",
    "            normalize=normalize_whitespace,\n",
    "        )\n",
    "\n",
    "\n",
    "def _clone_langchain_message_with_content(msg: Any, new_content: str) -> Any:\n",
    "    \"\"\"\n",
    "    Clone un message LangChain (BaseMessage) en remplaÃ§ant `content`, best-effort.\n",
    "\n",
    "    On Ã©vite d'importer LangChain Ã  l'import du module; l'import est fait ici si possible.\n",
    "\n",
    "    Args:\n",
    "        msg: objet message.\n",
    "        new_content: contenu final.\n",
    "\n",
    "    Returns:\n",
    "        nouveau message (ou fallback str si impossible).\n",
    "    \"\"\"\n",
    "    # cas simple: string\n",
    "    if isinstance(msg, str):\n",
    "        return new_content\n",
    "\n",
    "    # Tentatives LangChain (pydantic/dataclass)\n",
    "    try:\n",
    "        from langchain_core.messages import BaseMessage  # type: ignore\n",
    "        if isinstance(msg, BaseMessage):\n",
    "            # Pydantic v2\n",
    "            if hasattr(msg, \"model_copy\"):\n",
    "                return msg.model_copy(update={\"content\": new_content})\n",
    "            # Pydantic v1\n",
    "            if hasattr(msg, \"copy\"):\n",
    "                try:\n",
    "                    return msg.copy(update={\"content\": new_content})\n",
    "                except TypeError:\n",
    "                    return msg.copy()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Generic: tenter reconstruction via __class__(**fields)\n",
    "    try:\n",
    "        if hasattr(msg, \"model_dump\"):\n",
    "            d = msg.model_dump()\n",
    "        elif hasattr(msg, \"dict\"):\n",
    "            d = msg.dict()\n",
    "        elif dataclasses.is_dataclass(msg):\n",
    "            d = dataclasses.asdict(msg)\n",
    "        else:\n",
    "            d = dict(getattr(msg, \"__dict__\", {}))\n",
    "        d[\"content\"] = new_content\n",
    "        cls = msg.__class__\n",
    "        return cls(**d)\n",
    "    except Exception:\n",
    "        # fallback string\n",
    "        return new_content\n",
    "\n",
    "\n",
    "def wrap_prompt_builder_with_addendum(\n",
    "    prompt_fn: Callable[..., Any],\n",
    "    *,\n",
    "    store: TextOverrideStore,\n",
    "    addendum_key: str,\n",
    "    header: str = \"\\n\\n# Addendum\\n\",\n",
    ") -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    Wrap une fonction de prompt pour lui ajouter un \"addendum\" contrÃ´lÃ© par `store`.\n",
    "\n",
    "    - Si `store.get(addendum_key)` est vide â†’ comportement inchangÃ©.\n",
    "    - Sinon â†’ on concatÃ¨ne `original_content + header + addendum`.\n",
    "\n",
    "    CompatibilitÃ©:\n",
    "        - si la fonction renvoie un `str`, on renvoie un `str`\n",
    "        - si elle renvoie un message LangChain, on renvoie un message du mÃªme type (best-effort)\n",
    "\n",
    "    Args:\n",
    "        prompt_fn: fonction originale (ex: prompts.plan_prompt).\n",
    "        store: TextOverrideStore.\n",
    "        addendum_key: nom du param d'override.\n",
    "        header: sÃ©parateur ajoutÃ© avant l'addendum.\n",
    "\n",
    "    Returns:\n",
    "        fonction wrapper.\n",
    "    \"\"\"\n",
    "    def _wrapped(*args, **kwargs):\n",
    "        out = prompt_fn(*args, **kwargs)\n",
    "        add = store.get(addendum_key, \"\").strip()\n",
    "        if not add:\n",
    "            return out\n",
    "\n",
    "        # Extraire le contenu initial\n",
    "        if isinstance(out, str):\n",
    "            base = out\n",
    "        else:\n",
    "            base = str(getattr(out, \"content\", out))\n",
    "\n",
    "        new_content = base + header + add\n",
    "        return _clone_langchain_message_with_content(out, new_content)\n",
    "\n",
    "    # garder un minimum de metadata\n",
    "    try:\n",
    "        _wrapped.__name__ = getattr(prompt_fn, \"__name__\", \"prompt_wrapper\")\n",
    "        _wrapped.__doc__ = getattr(prompt_fn, \"__doc__\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return _wrapped\n",
    "\n",
    "\n",
    "def inject_params_into_otlp(\n",
    "    otlp: JSONDict,\n",
    "    param_specs: Sequence[ParamSpec],\n",
    "    *,\n",
    "    default_attach_to: Optional[SpanMatcher] = None,\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Ajoute des attributs `param.<name>` aux spans OTLP, selon les ParamSpec.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP (sera copiÃ©).\n",
    "        param_specs: liste des ParamSpec Ã  exposer.\n",
    "        default_attach_to: matcher fallback si ParamSpec.attach_to est None.\n",
    "\n",
    "    Returns:\n",
    "        copie modifiÃ©e du payload OTLP.\n",
    "    \"\"\"\n",
    "    otlp2 = copy.deepcopy(otlp)\n",
    "    for spec in param_specs:\n",
    "        matcher = spec.attach_to or default_attach_to\n",
    "        if matcher is None:\n",
    "            # pas d'endroit oÃ¹ accrocher => skip\n",
    "            continue\n",
    "        val = spec.value_as_str()\n",
    "        for sp in select_spans(otlp2, matcher):\n",
    "            otlp_set_span_attribute(sp, f\"param.{spec.name}\", val)\n",
    "            otlp_set_span_attribute(sp, f\"param.{spec.name}.trainable\", \"true\" if spec.trainable else \"false\")\n",
    "    return otlp2\n",
    "\n",
    "\n",
    "def add_evaluator_span(\n",
    "    otlp: JSONDict,\n",
    "    *,\n",
    "    score: float,\n",
    "    metrics: Mapping[str, float],\n",
    "    reasons: str = \"\",\n",
    "    parent_matcher: Optional[SpanMatcher] = None,\n",
    "    evaluator_span_name: str = \"evaluator\",\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Ajoute un span OTLP `evaluator` portant `eval.*` (score, mÃ©triques, raisons).\n",
    "\n",
    "    Important:\n",
    "        On n'attache PAS les `param.*` sur ce span (sauf si vous le dÃ©cidez),\n",
    "        pour Ã©viter l'optimisation boÃ®te noire.\n",
    "\n",
    "    Args:\n",
    "        otlp: payload OTLP (copiÃ©).\n",
    "        score: score global (0..1).\n",
    "        metrics: dict mÃ©triques (0..1).\n",
    "        reasons: texte explicatif.\n",
    "        parent_matcher: oÃ¹ accrocher l'evaluator (typiquement le span \"synthesizer\").\n",
    "        evaluator_span_name: nom de span.\n",
    "\n",
    "    Returns:\n",
    "        payload OTLP modifiÃ©.\n",
    "    \"\"\"\n",
    "    otlp2 = copy.deepcopy(otlp)\n",
    "    trace_id = otlp_get_trace_id(otlp2) or _new_trace_id_hex()\n",
    "\n",
    "    # choisir le parent span id\n",
    "    parent_span_id = \"\"\n",
    "    if parent_matcher is not None:\n",
    "        parent = select_one_span(otlp2, parent_matcher)\n",
    "        if parent is not None:\n",
    "            parent_span_id = str(parent.get(\"spanId\") or \"\")\n",
    "\n",
    "    # fallback: dernier span par endTimeUnixNano\n",
    "    if not parent_span_id:\n",
    "        spans = list(otlp_iter_spans(otlp2))\n",
    "        if spans:\n",
    "            spans_sorted = sorted(spans, key=lambda s: int(s.get(\"endTimeUnixNano\") or 0))\n",
    "            parent_span_id = str(spans_sorted[-1].get(\"spanId\") or \"\")\n",
    "\n",
    "    now_ns = time.time_ns()\n",
    "    span: SpanDict = {\n",
    "        \"traceId\": trace_id,\n",
    "        \"spanId\": _new_span_id_hex(),\n",
    "        \"parentSpanId\": parent_span_id,\n",
    "        \"name\": evaluator_span_name,\n",
    "        \"kind\": \"INTERNAL\",\n",
    "        \"startTimeUnixNano\": int(now_ns),\n",
    "        \"endTimeUnixNano\": int(now_ns + 500_000),\n",
    "        \"attributes\": [\n",
    "            _otlp_kv(\"eval.score\", str(float(score))),\n",
    "            _otlp_kv(\"eval.reasons\", reasons or \"\"),\n",
    "        ],\n",
    "    }\n",
    "    for k, v in metrics.items():\n",
    "        span[\"attributes\"].append(_otlp_kv(f\"eval.{k}\", str(float(v))))\n",
    "    # Optionnel: input/output.value pour donner de la \"matiÃ¨re\" au graphe\n",
    "    span[\"attributes\"].append(_otlp_kv(\"input.value\", \"TruLens feedback\"))\n",
    "    span[\"attributes\"].append(_otlp_kv(\"output.value\", reasons or \"\"))\n",
    "\n",
    "    otlp_append_span(otlp2, span)\n",
    "    return otlp2\n",
    "\n",
    "\n",
    "def coerce_to_otlp(\n",
    "    trace_or_record: Any,\n",
    "    *,\n",
    "    service_name: str = \"app\",\n",
    "    scope_name: str = \"trace_opt\",\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Convertit une entrÃ©e \"trace-like\" en OTLP.\n",
    "\n",
    "    Supporte:\n",
    "      - payload OTLP natif (dict avec `resourceSpans`)\n",
    "      - Record TruLens JSON (dict avec `calls` / `record_id`) -> OTLP minimal\n",
    "\n",
    "    Args:\n",
    "        trace_or_record: OTLP ou Record TruLens.\n",
    "        service_name: utilisÃ© si conversion TruLens -> OTLP.\n",
    "        scope_name: utilisÃ© si conversion TruLens -> OTLP.\n",
    "\n",
    "    Returns:\n",
    "        payload OTLP.\n",
    "    \"\"\"\n",
    "    if otlp_is_payload(trace_or_record):\n",
    "        return trace_or_record  # type: ignore[return-value]\n",
    "    if trulens_is_record(trace_or_record):\n",
    "        return trulens_record_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)  # type: ignore[arg-type]\n",
    "    raise ValueError(\"EntrÃ©e non reconnue: attendu OTLP ou Record TruLens JSON.\")\n",
    "\n",
    "\n",
    "def param_descriptions_from_specs(param_specs: Sequence[ParamSpec]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Construit un mapping semantic_name -> description Ã  partir des ParamSpec.\n",
    "\n",
    "    Astuce:\n",
    "        `semantic_name` correspond au nom de param tel qu'il apparaÃ®t dans Trace\n",
    "        (sans prÃ©fixe runX:).\n",
    "\n",
    "    Args:\n",
    "        param_specs: specs.\n",
    "\n",
    "    Returns:\n",
    "        dict.\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    for s in param_specs:\n",
    "        if s.description:\n",
    "            out[s.name] = s.description\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_otlp_for_optimizer(\n",
    "    trace_or_record: Any,\n",
    "    *,\n",
    "    param_specs: Sequence[ParamSpec],\n",
    "    score: float,\n",
    "    metrics: Mapping[str, float],\n",
    "    reasons: str = \"\",\n",
    "    default_param_attach_to: Optional[SpanMatcher] = None,\n",
    "    evaluator_parent_matcher: Optional[SpanMatcher] = None,\n",
    "    service_name: str = \"app\",\n",
    "    scope_name: str = \"trace_opt\",\n",
    ") -> JSONDict:\n",
    "    \"\"\"\n",
    "    Pipeline \"one-shot\" : (trace|record) -> OTLP -> inject params -> add evaluator.\n",
    "\n",
    "    Args:\n",
    "        trace_or_record: OTLP ou Record TruLens.\n",
    "        param_specs: paramÃ¨tres trainables Ã  exposer.\n",
    "        score: score global.\n",
    "        metrics: dict mÃ©triques.\n",
    "        reasons: texte explicatif.\n",
    "        default_param_attach_to: fallback pour ParamSpec.attach_to.\n",
    "        evaluator_parent_matcher: span parent pour l'evaluator.\n",
    "        service_name: service.name si conversion TruLens -> OTLP.\n",
    "        scope_name: scope.name si conversion TruLens -> OTLP.\n",
    "\n",
    "    Returns:\n",
    "        payload OTLP prÃªt Ã  Ãªtre ingÃ©rÃ© dans Trace.\n",
    "    \"\"\"\n",
    "    otlp0 = coerce_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)\n",
    "    otlp1 = inject_params_into_otlp(otlp0, param_specs, default_attach_to=default_param_attach_to)\n",
    "    otlp2 = add_evaluator_span(\n",
    "        otlp1,\n",
    "        score=score,\n",
    "        metrics=metrics,\n",
    "        reasons=reasons,\n",
    "        parent_matcher=evaluator_parent_matcher,\n",
    "    )\n",
    "    return otlp2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Extraction mÃ©triques TruLens (depuis DataFrame row OU JSON)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def extract_metrics_from_mapping(\n",
    "    obj: Mapping[str, Any],\n",
    "    *,\n",
    "    metric_keys: Sequence[str],\n",
    "    default_metric: float = 0.5,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extrait des mÃ©triques depuis un mapping (dict-like) via des clÃ©s.\n",
    "\n",
    "    Args:\n",
    "        obj: mapping (ex: row.to_dict()).\n",
    "        metric_keys: noms de colonnes / champs.\n",
    "        default_metric: fallback si manquant.\n",
    "\n",
    "    Returns:\n",
    "        dict mÃ©trique -> float.\n",
    "    \"\"\"\n",
    "    out: Dict[str, float] = {}\n",
    "    for k in metric_keys:\n",
    "        val = obj.get(k, default_metric)\n",
    "        try:\n",
    "            out[k] = float(val)\n",
    "        except Exception:\n",
    "            out[k] = float(default_metric)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_score(\n",
    "    metrics: Mapping[str, float],\n",
    "    *,\n",
    "    weights: Optional[Mapping[str, float]] = None,\n",
    "    clamp_0_1: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcule un score scalaire Ã  partir d'un dict de mÃ©triques.\n",
    "\n",
    "    Args:\n",
    "        metrics: dict mÃ©trique -> float.\n",
    "        weights: dict mÃ©trique -> poids (sinon moyenne uniforme).\n",
    "        clamp_0_1: clamp le rÃ©sultat entre [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        float score.\n",
    "    \"\"\"\n",
    "    if not metrics:\n",
    "        return 0.5\n",
    "    if weights:\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for k, v in metrics.items():\n",
    "            w = float(weights.get(k, 0.0))\n",
    "            num += w * float(v)\n",
    "            den += w\n",
    "        score = num / den if den > 0 else sum(float(v) for v in metrics.values()) / len(metrics)\n",
    "    else:\n",
    "        score = sum(float(v) for v in metrics.values()) / len(metrics)\n",
    "    if clamp_0_1:\n",
    "        score = max(0.0, min(1.0, score))\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def select_latest_item(container: Any) -> Any:\n",
    "    \"\"\"\n",
    "    SÃ©lectionne \"le dernier Ã©lÃ©ment\" d'un container.\n",
    "\n",
    "    Supporte:\n",
    "      - pandas.DataFrame / pandas.Series via `.iloc[-1]`\n",
    "      - list/tuple via `[-1]`\n",
    "      - dict: renvoie tel quel (considÃ©rÃ© dÃ©jÃ  comme 1 record)\n",
    "\n",
    "    Args:\n",
    "        container: objet.\n",
    "\n",
    "    Returns:\n",
    "        dernier Ã©lÃ©ment ou l'objet lui-mÃªme (dict).\n",
    "\n",
    "    Raises:\n",
    "        ValueError si vide/incompatible.\n",
    "    \"\"\"\n",
    "    if container is None:\n",
    "        raise ValueError(\"container is None\")\n",
    "\n",
    "    if isinstance(container, dict):\n",
    "        return container\n",
    "\n",
    "    # pandas DataFrame/Series\n",
    "    if hasattr(container, \"iloc\"):\n",
    "        try:\n",
    "            if getattr(container, \"shape\", (0,))[0] == 0:\n",
    "                raise ValueError(\"container is empty\")\n",
    "            return container.iloc[-1]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if isinstance(container, (list, tuple)):\n",
    "        if not container:\n",
    "            raise ValueError(\"container is empty\")\n",
    "        return container[-1]\n",
    "\n",
    "    raise ValueError(f\"Type non supportÃ© pour select_latest_item: {type(container)}\")\n",
    "\n",
    "\n",
    "def extract_mapping(obj: Any) -> Mapping[str, Any]:\n",
    "    \"\"\"\n",
    "    Convertit best-effort un objet en mapping (dict-like).\n",
    "\n",
    "    Supporte:\n",
    "      - dict: renvoie tel quel\n",
    "      - pandas.Series: `.to_dict()`\n",
    "      - objets avec `model_dump()` (pydantic v2) ou `dict()` (pydantic v1)\n",
    "\n",
    "    Args:\n",
    "        obj: objet.\n",
    "\n",
    "    Returns:\n",
    "        mapping.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "\n",
    "    if hasattr(obj, \"to_dict\"):\n",
    "        try:\n",
    "            return obj.to_dict()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(obj, \"model_dump\"):\n",
    "        try:\n",
    "            return obj.model_dump()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(obj, \"dict\"):\n",
    "        try:\n",
    "            return obj.dict()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # fallback\n",
    "    return {\"value\": obj}\n",
    "\n",
    "\n",
    "def extract_trulens_record_json(obj: Any) -> Optional[JSONDict]:\n",
    "    \"\"\"\n",
    "    Extrait un Record TruLens JSON depuis diffÃ©rents conteneurs.\n",
    "\n",
    "    Cas gÃ©rÃ©s:\n",
    "      - si `obj` est dÃ©jÃ  un record dict (trulens_is_record) -> renvoie obj\n",
    "      - si `obj` est une row (Series/dict) contenant un champ `record_json` ou `record`\n",
    "        (dict ou JSON str) -> parse et renvoie.\n",
    "      - sinon None\n",
    "\n",
    "    Args:\n",
    "        obj: record-like.\n",
    "\n",
    "    Returns:\n",
    "        dict record ou None.\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(obj, dict) and trulens_is_record(obj):\n",
    "        return obj\n",
    "\n",
    "    m = extract_mapping(obj)\n",
    "\n",
    "    for key in (\"record_json\", \"record\", \"record_jsons\", \"record_json_str\"):\n",
    "        if key in m:\n",
    "            raw = m.get(key)\n",
    "            if isinstance(raw, dict) and trulens_is_record(raw):\n",
    "                return raw\n",
    "            if isinstance(raw, str):\n",
    "                try:\n",
    "                    parsed = json.loads(raw)\n",
    "                    if isinstance(parsed, dict) and trulens_is_record(parsed):\n",
    "                        return parsed\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # parfois le record est stockÃ© sous une clÃ© \"calls\" + \"record_id\" etc.\n",
    "    if isinstance(m, dict) and trulens_is_record(m):\n",
    "        return dict(m)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def render_feedback_text(\n",
    "    *,\n",
    "    score: float,\n",
    "    metrics: Mapping[str, float],\n",
    "    reasons: str = \"\",\n",
    "    extra: Optional[Mapping[str, Any]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Rend un texte de feedback (Ã  passer Ã  OptoPrime) Ã  partir du score/mÃ©triques.\n",
    "\n",
    "    Args:\n",
    "        score: score global.\n",
    "        metrics: dict mÃ©triques.\n",
    "        reasons: texte explicatif (si dispo).\n",
    "        extra: infos additionnelles (ex: query, output, etc).\n",
    "\n",
    "    Returns:\n",
    "        str.\n",
    "    \"\"\"\n",
    "    parts = [f\"score={score:.3f}\"]\n",
    "    if metrics:\n",
    "        parts.append(\"metrics=\" + \", \".join(f\"{k}={v:.3f}\" for k, v in metrics.items()))\n",
    "    if reasons:\n",
    "        parts.append(\"reasons=\" + reasons.strip())\n",
    "    if extra:\n",
    "        for k, v in extra.items():\n",
    "            parts.append(f\"{k}={safe_json_dumps(v, max_len=600)}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Code targets / patching (optimisation de code)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class CodeTarget:\n",
    "    \"\"\"\n",
    "    Cible de patching pour l'optimisation de code.\n",
    "\n",
    "    Un CodeTarget est associÃ© Ã  un paramÃ¨tre trainable :\n",
    "        param.__code_<key>\n",
    "\n",
    "    Attributes:\n",
    "        key: identifiant stable (ex: \"planner_node\").\n",
    "        get_callable: fonction retournant l'objet callable courant Ã  patcher.\n",
    "        set_callable: optionnel, pour remplacer le symbole (module.attr = new_fn).\n",
    "        attach_to: SpanMatcher oÃ¹ accrocher le paramÃ¨tre code dans la trace.\n",
    "        trainable: bool.\n",
    "        description: aide l'optimiseur (ex: signature / rÃ´le).\n",
    "    \"\"\"\n",
    "    key: str\n",
    "    get_callable: Callable[[], Callable[..., Any]]\n",
    "    set_callable: Optional[Callable[[Callable[..., Any]], None]] = None\n",
    "    attach_to: Optional[SpanMatcher] = None\n",
    "    trainable: bool = True\n",
    "    description: str = \"\"\n",
    "\n",
    "    @property\n",
    "    def param_name(self) -> str:\n",
    "        \"\"\"Nom du paramÃ¨tre exposÃ© dans la trace.\"\"\"\n",
    "        return f\"__code_{self.key}\"\n",
    "\n",
    "    def get_source(self) -> str:\n",
    "        \"\"\"\n",
    "        Extrait le code source de la fonction cible via inspect.getsource.\n",
    "\n",
    "        Returns:\n",
    "            str code python.\n",
    "        \"\"\"\n",
    "        fn = self.get_callable()\n",
    "        try:\n",
    "            return inspect.getsource(fn)\n",
    "        except OSError:\n",
    "            # ex: fonctions dÃ©finies dans un notebook sans source dispo\n",
    "            return f\"# Source indisponible pour {getattr(fn, '__name__', self.key)}\\n\"\n",
    "\n",
    "    def infer_description(self) -> str:\n",
    "        \"\"\"\n",
    "        DÃ©duit une description courte si `description` n'est pas fourni.\n",
    "\n",
    "        Returns:\n",
    "            str.\n",
    "        \"\"\"\n",
    "        if self.description:\n",
    "            return self.description\n",
    "        fn = self.get_callable()\n",
    "        try:\n",
    "            sig = str(inspect.signature(fn))\n",
    "        except Exception:\n",
    "            sig = \"(...)\"\n",
    "        return f\"{getattr(fn, '__name__', self.key)}{sig}\"\n",
    "\n",
    "\n",
    "def hotpatch_function_in_place(target_fn: Callable[..., Any], new_fn: Callable[..., Any]) -> None:\n",
    "    \"\"\"\n",
    "    Hotpatch \"in-place\" : remplace le bytecode (`__code__`) de `target_fn` par celui de `new_fn`.\n",
    "\n",
    "    Avantage:\n",
    "        Si LangGraph a capturÃ© une *rÃ©fÃ©rence* vers `target_fn`, le patch est effectif\n",
    "        sans recompiler le graphe.\n",
    "\n",
    "    Limites:\n",
    "        Ne marche pas si la fonction utilise des closures incompatibles.\n",
    "\n",
    "    Args:\n",
    "        target_fn: fonction originale (objet) utilisÃ©e par le graphe.\n",
    "        new_fn: fonction compilÃ©e Ã  partir d'un nouveau source.\n",
    "\n",
    "    Raises:\n",
    "        TypeError si pas patchable.\n",
    "    \"\"\"\n",
    "    if not (isinstance(target_fn, types.FunctionType) and isinstance(new_fn, types.FunctionType)):\n",
    "        raise TypeError(\"hotpatch_function_in_place ne supporte que des functions Python.\")\n",
    "    target_fn.__code__ = new_fn.__code__\n",
    "    target_fn.__defaults__ = new_fn.__defaults__\n",
    "    target_fn.__kwdefaults__ = new_fn.__kwdefaults__\n",
    "    target_fn.__annotations__ = getattr(new_fn, \"__annotations__\", {})\n",
    "    target_fn.__doc__ = getattr(new_fn, \"__doc__\", None)\n",
    "\n",
    "\n",
    "def compile_function_from_source(source: str, fn_name: str, *, glb: Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    Compile un source python contenant une dÃ©finition `def <fn_name>(...)` et renvoie cette fonction.\n",
    "\n",
    "    Args:\n",
    "        source: code python (doit dÃ©finir fn_name).\n",
    "        fn_name: nom de la fonction Ã  extraire.\n",
    "        glb: globals Ã  utiliser (permet d'accÃ©der aux imports existants).\n",
    "\n",
    "    Returns:\n",
    "        function object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError si la fonction n'existe pas aprÃ¨s exec.\n",
    "    \"\"\"\n",
    "    glb = glb or {}\n",
    "    loc: Dict[str, Any] = {}\n",
    "    compiled = compile(source, \"<optimized>\", \"exec\")\n",
    "    exec(compiled, glb, loc)\n",
    "    fn = loc.get(fn_name) or glb.get(fn_name)\n",
    "    if not callable(fn):\n",
    "        raise ValueError(f\"Le source ne dÃ©finit pas la fonction attendue: {fn_name}\")\n",
    "    return fn  # type: ignore\n",
    "\n",
    "\n",
    "def apply_code_update(\n",
    "    *,\n",
    "    update_source: str,\n",
    "    target: CodeTarget,\n",
    "    patch_mode: str = \"in_place_or_replace\",\n",
    "    global_ns: Optional[Dict[str, Any]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Applique un patch de code produit par l'optimiseur Ã  une cible.\n",
    "\n",
    "    Modes:\n",
    "      - \"in_place\": hotpatch sur l'objet callable actuel uniquement.\n",
    "      - \"replace\": remplace le symbole via target.set_callable (ou error si absent).\n",
    "      - \"in_place_or_replace\": tente in_place, sinon fallback replace.\n",
    "      - \"replace_and_in_place\": fait replace puis hotpatch (utile si le graphe a capturÃ© l'ancien objet).\n",
    "\n",
    "    Args:\n",
    "        update_source: code python complet (def ...).\n",
    "        target: CodeTarget.\n",
    "        patch_mode: stratÃ©gie.\n",
    "        global_ns: dict globals pour exec (souvent globals()).\n",
    "\n",
    "    Raises:\n",
    "        Exception si impossible.\n",
    "    \"\"\"\n",
    "    fn0 = target.get_callable()\n",
    "    fn_name = getattr(fn0, \"__name__\", None) or target.key\n",
    "    global_ns = global_ns or {}\n",
    "\n",
    "    new_fn = compile_function_from_source(update_source, fn_name, glb=global_ns)\n",
    "\n",
    "    if patch_mode == \"in_place\":\n",
    "        hotpatch_function_in_place(fn0, new_fn)\n",
    "        return\n",
    "\n",
    "    if patch_mode == \"replace\":\n",
    "        if target.set_callable is None:\n",
    "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
    "        target.set_callable(new_fn)\n",
    "        return\n",
    "\n",
    "    if patch_mode == \"replace_and_in_place\":\n",
    "        if target.set_callable is None:\n",
    "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
    "        target.set_callable(new_fn)\n",
    "        # tenter hotpatch sur l'ancien objet\n",
    "        try:\n",
    "            hotpatch_function_in_place(fn0, new_fn)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    # in_place_or_replace\n",
    "    try:\n",
    "        hotpatch_function_in_place(fn0, new_fn)\n",
    "        return\n",
    "    except Exception:\n",
    "        if target.set_callable is None:\n",
    "            raise\n",
    "        target.set_callable(new_fn)\n",
    "        return\n",
    "\n",
    "\n",
    "def build_code_param_specs(code_targets: Sequence[CodeTarget]) -> List[ParamSpec]:\n",
    "    \"\"\"\n",
    "    Convertit des CodeTarget en ParamSpec (pour injection OTLP + updates).\n",
    "\n",
    "    Args:\n",
    "        code_targets: cibles de code.\n",
    "\n",
    "    Returns:\n",
    "        liste ParamSpec.\n",
    "    \"\"\"\n",
    "    specs: List[ParamSpec] = []\n",
    "    for t in code_targets:\n",
    "        # closure pour get_source\n",
    "        def _make_getter(tt: CodeTarget) -> Callable[[], Any]:\n",
    "            return lambda: tt.get_source()\n",
    "\n",
    "        def _make_applier(tt: CodeTarget) -> Callable[[str], None]:\n",
    "            return lambda src: apply_code_update(update_source=src, target=tt, patch_mode=\"in_place_or_replace\", global_ns=globals())\n",
    "\n",
    "        specs.append(\n",
    "            ParamSpec(\n",
    "                name=t.param_name,\n",
    "                get_value=_make_getter(t),\n",
    "                apply_update=_make_applier(t),\n",
    "                attach_to=t.attach_to,\n",
    "                trainable=t.trainable,\n",
    "                description=t.infer_description(),\n",
    "                normalize=normalize_whitespace,\n",
    "            )\n",
    "        )\n",
    "    return specs\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Optimisation Trace/OptoPrime (Ã  partir d'OTLP)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Petits wrappers autour des imports Trace pour rester optionnels\n",
    "def _require_trace_imports():\n",
    "    \"\"\"\n",
    "    Importe dynamiquement les composants Trace nÃ©cessaires.\n",
    "\n",
    "    Raises:\n",
    "        ImportError si la lib Trace/opto n'est pas installÃ©e/disponible.\n",
    "    \"\"\"\n",
    "    from opto.trace.io.otel_adapter import otlp_traces_to_trace_json  # type: ignore\n",
    "    from opto.trace.io.tgj_ingest import ingest_tgj  # type: ignore\n",
    "    from opto.trace.nodes import MessageNode, ParameterNode  # type: ignore\n",
    "    from opto.optimizers import OptoPrimeV2  # type: ignore\n",
    "    from opto.optimizers.utils import OptimizerPromptSymbolSetJSON  # type: ignore\n",
    "    from opto.trainer.algorithms.basic_algorithms import batchify  # type: ignore\n",
    "\n",
    "    return otlp_traces_to_trace_json, ingest_tgj, MessageNode, ParameterNode, OptoPrimeV2, OptimizerPromptSymbolSetJSON, batchify\n",
    "\n",
    "\n",
    "def find_target(nodes: Dict[str, Any], *, prefer_name_contains: str = \"evaluator\") -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Trouve le nÅ“ud cible (MessageNode) Ã  optimiser.\n",
    "\n",
    "    Heuristique:\n",
    "      - si un MessageNode contient `prefer_name_contains` dans son nom â†’ on le prend\n",
    "      - sinon, on prend le \"dernier\" MessageNode rencontrÃ©.\n",
    "\n",
    "    Args:\n",
    "        nodes: dict name->node (rÃ©sultat ingest_tgj).\n",
    "        prefer_name_contains: substring.\n",
    "\n",
    "    Returns:\n",
    "        MessageNode ou None.\n",
    "    \"\"\"\n",
    "    _, _, MessageNode, _, _, _, _ = _require_trace_imports()\n",
    "    last = None\n",
    "    for n in nodes.values():\n",
    "        if isinstance(n, MessageNode):\n",
    "            last = n\n",
    "            if prefer_name_contains.lower() in (n.name or \"\").lower():\n",
    "                return n\n",
    "    return last\n",
    "\n",
    "\n",
    "def visualize_graph(nodes: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Visualise un graphe Trace (paramÃ¨tres + messages) sous forme texte.\n",
    "\n",
    "    Args:\n",
    "        nodes: dict name->node.\n",
    "\n",
    "    Returns:\n",
    "        str multi-ligne.\n",
    "    \"\"\"\n",
    "    _, _, MessageNode, ParameterNode, _, _, _ = _require_trace_imports()\n",
    "    params = []\n",
    "    messages = []\n",
    "    for name, node in nodes.items():\n",
    "        if isinstance(node, ParameterNode):\n",
    "            data = getattr(node, \"data\", \"\")\n",
    "            data_s = data[:80] + (\"...\" if isinstance(data, str) and len(data) > 80 else \"\")\n",
    "            params.append(f\"[PARAM] {node.name}: {data_s!r}\")\n",
    "        elif isinstance(node, MessageNode):\n",
    "            parents = getattr(node, \"parents\", []) or []\n",
    "            parent_names = [getattr(p, \"name\", \"?\") for p in parents]\n",
    "            messages.append(f\"[MSG] {node.name} â† {parent_names if parent_names else 'ROOT'}\")\n",
    "    return \"\\n\".join(params + messages)\n",
    "\n",
    "\n",
    "def check_reachability(target: Any, params: List[Any]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    VÃ©rifie si chaque paramÃ¨tre est atteignable depuis `target` via les parents.\n",
    "\n",
    "    Utile pour dÃ©tecter un paramÃ¨tre accrochÃ© Ã  un span \"isolÃ©\" (non causalement reliÃ©).\n",
    "\n",
    "    Args:\n",
    "        target: MessageNode cible.\n",
    "        params: liste de ParameterNode.\n",
    "\n",
    "    Returns:\n",
    "        dict param.name -> bool.\n",
    "    \"\"\"\n",
    "    _, _, _, ParameterNode, _, _, _ = _require_trace_imports()\n",
    "    seen = set()\n",
    "    stack = [target]\n",
    "    reachable = set()\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if hasattr(node, \"parents\"):\n",
    "            for p in getattr(node, \"parents\") or []:\n",
    "                if p not in seen:\n",
    "                    stack.append(p)\n",
    "        if isinstance(node, ParameterNode):\n",
    "            reachable.add(node.name)\n",
    "    return {p.name: p.name in reachable for p in params}\n",
    "\n",
    "\n",
    "def _remap_params_in_graph(node: Any, param_mapping: Dict[int, Any], visited=None) -> None:\n",
    "    \"\"\"\n",
    "    Remappe rÃ©cursivement des ParameterNode dans un graphe Trace.\n",
    "\n",
    "    Lorsqu'on rÃ©utilise un optimiseur entre itÃ©rations, on veut que les graphs\n",
    "    utilisent *les mÃªmes objets ParameterNode* (ceux de l'optimiseur), sinon\n",
    "    l'optimiseur considÃ¨re des params diffÃ©rents.\n",
    "\n",
    "    Args:\n",
    "        node: nÅ“ud courant.\n",
    "        param_mapping: dict id(old_param) -> optimizer_param.\n",
    "        visited: set d'ids dÃ©jÃ  visitÃ©s.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    node_id = id(node)\n",
    "    if node_id in visited:\n",
    "        return\n",
    "    visited.add(node_id)\n",
    "\n",
    "    # Remap inputs\n",
    "    if hasattr(node, \"_inputs\") and isinstance(getattr(node, \"_inputs\"), dict):\n",
    "        inputs = getattr(node, \"_inputs\")\n",
    "        for key, input_node in list(inputs.items()):\n",
    "            in_id = id(input_node)\n",
    "            if in_id in param_mapping:\n",
    "                inputs[key] = param_mapping[in_id]\n",
    "            else:\n",
    "                _remap_params_in_graph(input_node, param_mapping, visited)\n",
    "\n",
    "    # Remap parents list\n",
    "    if hasattr(node, \"parents\") and isinstance(getattr(node, \"parents\"), list):\n",
    "        parents = getattr(node, \"parents\")\n",
    "        for i, parent in enumerate(list(parents)):\n",
    "            p_id = id(parent)\n",
    "            if p_id in param_mapping:\n",
    "                parents[i] = param_mapping[p_id]\n",
    "            else:\n",
    "                _remap_params_in_graph(parent, param_mapping, visited)\n",
    "\n",
    "\n",
    "def show_prompt_diff(before: str, after: str, *, context_lines: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Produit un diff textuel compact pour des prompts (ou code).\n",
    "\n",
    "    Args:\n",
    "        before: texte original.\n",
    "        after: texte modifiÃ©.\n",
    "        context_lines: lignes de contexte.\n",
    "\n",
    "    Returns:\n",
    "        diff str.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    before_lines = normalize_whitespace(before).splitlines(True)\n",
    "    after_lines = normalize_whitespace(after).splitlines(True)\n",
    "    diff = difflib.unified_diff(before_lines, after_lines, fromfile=\"before\", tofile=\"after\", n=context_lines)\n",
    "    return \"\".join(diff)\n",
    "\n",
    "\n",
    "def compute_change_stats(before: str, after: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcule des statistiques simples sur un changement (longueur, delta, etc).\n",
    "\n",
    "    Args:\n",
    "        before: texte original.\n",
    "        after: texte modifiÃ©.\n",
    "\n",
    "    Returns:\n",
    "        dict stats.\n",
    "    \"\"\"\n",
    "    b = before or \"\"\n",
    "    a = after or \"\"\n",
    "    return {\n",
    "        \"len_before\": len(b),\n",
    "        \"len_after\": len(a),\n",
    "        \"delta\": len(a) - len(b),\n",
    "        \"delta_pct\": ((len(a) - len(b)) / len(b) * 100.0) if len(b) else None,\n",
    "        \"lines_before\": b.count(\"\\n\") + 1 if b else 0,\n",
    "        \"lines_after\": a.count(\"\\n\") + 1 if a else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def _ensure_param_descriptions_on_optimizer(optimizer: Any, params: Sequence[Any], desc_by_name: Mapping[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Ajoute/complÃ¨te les descriptions de paramÃ¨tres cÃ´tÃ© optimiseur (si le champ existe).\n",
    "\n",
    "    Args:\n",
    "        optimizer: OptoPrimeV2.\n",
    "        params: ParameterNode (de l'optimiseur).\n",
    "        desc_by_name: mapping param_name -> description.\n",
    "    \"\"\"\n",
    "    # OptoPrime garde des params avec attributs name/data/desc (selon versions).\n",
    "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
    "        full_name = getattr(p, \"name\", \"\")\n",
    "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
    "        if semantic_name in desc_by_name:\n",
    "            if not getattr(p, \"desc\", \"\"):\n",
    "                try:\n",
    "                    p.desc = desc_by_name[semantic_name]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunResult:\n",
    "    \"\"\"\n",
    "    RÃ©sultat d'un run Ã  optimiser.\n",
    "\n",
    "    Attributes:\n",
    "        otlp: payload OTLP.\n",
    "        score: score global.\n",
    "        metrics: dict mÃ©triques.\n",
    "        feedback: texte de feedback (utilisÃ© par l'optimiseur).\n",
    "        meta: infos additionnelles (query, output, etc).\n",
    "    \"\"\"\n",
    "    otlp: JSONDict\n",
    "    score: float\n",
    "    metrics: Dict[str, float]\n",
    "    feedback: str\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "\n",
    "def optimize_iteration(\n",
    "    runs: Sequence[RunResult],\n",
    "    *,\n",
    "    optimizer: Optional[Any],\n",
    "    llm_client: Any,\n",
    "    objective: str,\n",
    "    param_name_substrings: Sequence[str] = (\"__code_\",),\n",
    "    memory_size: int = 12,\n",
    "    verbose_graph: bool = False,\n",
    "    param_descriptions: Optional[Mapping[str, str]] = None,\n",
    "    prefer_target_name_contains: str = \"evaluator\",\n",
    ") -> Tuple[Dict[str, str], Any]:\n",
    "    \"\"\"\n",
    "    ExÃ©cute une itÃ©ration OptoPrime sur un batch de runs.\n",
    "\n",
    "    Points importants (par rapport aux dÃ©mos) :\n",
    "      - compatible multi-runs (plusieurs requÃªtes) via batchify\n",
    "      - **remap** des ParameterNode quand on rÃ©utilise un optimiseur entre itÃ©rations,\n",
    "        afin que les nouveaux graphs pointent vers les *mÃªmes objets* paramÃ¨tres\n",
    "      - filtre simple sur les paramÃ¨tres Ã  optimiser via `param_name_substrings`\n",
    "\n",
    "    Args:\n",
    "        runs: liste de runs (idÃ©alement plusieurs requÃªtes pour un signal plus robuste).\n",
    "        optimizer: OptoPrimeV2 existant (ou None au 1er tour).\n",
    "        llm_client: client LLM pour l'optimiseur (comme dans les dÃ©mos).\n",
    "        objective: instruction globale (\"maximize eval.score ...\").\n",
    "        param_name_substrings: filtre sur le champ `ParameterNode.name`.\n",
    "        memory_size: mÃ©moire de l'optimiseur.\n",
    "        verbose_graph: si True, imprime une visualisation texte des graphs.\n",
    "        param_descriptions: mapping semantic_name -> description (optionnel).\n",
    "        prefer_target_name_contains: substring pour choisir la cible (default \"evaluator\").\n",
    "\n",
    "    Returns:\n",
    "        (updates, optimizer)\n",
    "        updates: dict semantic_param_name -> new_value\n",
    "    \"\"\"\n",
    "    (\n",
    "        otlp_traces_to_trace_json,\n",
    "        ingest_tgj,\n",
    "        MessageNode,\n",
    "        ParameterNode,\n",
    "        OptoPrimeV2,\n",
    "        OptimizerPromptSymbolSetJSON,\n",
    "        batchify,\n",
    "    ) = _require_trace_imports()\n",
    "\n",
    "    # Mapping semantic_name -> optimizer ParameterNode (si optimizer dÃ©jÃ  crÃ©Ã©)\n",
    "    opt_params_by_semantic: Dict[str, Any] = {}\n",
    "    if optimizer is not None:\n",
    "        for p in getattr(optimizer, \"parameters\", []) or []:\n",
    "            full = getattr(p, \"name\", \"\") or \"\"\n",
    "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
    "            opt_params_by_semantic[semantic] = p\n",
    "\n",
    "    all_targets: List[Any] = []\n",
    "    all_feedbacks: List[str] = []\n",
    "    iter_params_by_semantic: Dict[str, Any] = {}\n",
    "\n",
    "    for i, run in enumerate(runs):\n",
    "        tgj_docs = list(\n",
    "            otlp_traces_to_trace_json(\n",
    "                run.otlp,\n",
    "                agent_id_hint=f\"run{i}\",\n",
    "                use_temporal_hierarchy=True,\n",
    "            )\n",
    "        )\n",
    "        if not tgj_docs:\n",
    "            continue\n",
    "        nodes = ingest_tgj(tgj_docs[0])\n",
    "\n",
    "        target = find_target(nodes, prefer_name_contains=prefer_target_name_contains)\n",
    "        if target is None:\n",
    "            continue\n",
    "\n",
    "        # ParamÃ¨tres trainables filtrÃ©s\n",
    "        params_in_graph: List[Any] = []\n",
    "        for n in nodes.values():\n",
    "            if isinstance(n, ParameterNode) and getattr(n, \"trainable\", False):\n",
    "                nname = getattr(n, \"name\", \"\") or \"\"\n",
    "                if any(sub in nname for sub in param_name_substrings):\n",
    "                    params_in_graph.append(n)\n",
    "\n",
    "        # Remap vers optimizer params si possible\n",
    "        id_mapping: Dict[int, Any] = {}\n",
    "        new_params_to_add: List[Any] = []\n",
    "        for p in params_in_graph:\n",
    "            full = getattr(p, \"name\", \"\") or \"\"\n",
    "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
    "            if semantic in opt_params_by_semantic:\n",
    "                id_mapping[id(p)] = opt_params_by_semantic[semantic]\n",
    "                iter_params_by_semantic.setdefault(semantic, opt_params_by_semantic[semantic])\n",
    "            else:\n",
    "                # nouveau paramÃ¨tre jamais vu\n",
    "                iter_params_by_semantic.setdefault(semantic, p)\n",
    "                new_params_to_add.append(p)\n",
    "\n",
    "        if id_mapping:\n",
    "            _remap_params_in_graph(target, id_mapping)\n",
    "\n",
    "        # si optimizer existe, on lui ajoute les nouveaux paramÃ¨tres\n",
    "        if optimizer is not None:\n",
    "            for p in new_params_to_add:\n",
    "                optimizer.parameters.append(p)  # type: ignore[attr-defined]\n",
    "                full = getattr(p, \"name\", \"\") or \"\"\n",
    "                semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
    "                opt_params_by_semantic[semantic] = p\n",
    "\n",
    "        if verbose_graph:\n",
    "            print(\"\\n--- Graph (run\", i, \") ---\")\n",
    "            print(visualize_graph(nodes))\n",
    "\n",
    "        # Reachability diagnostic (aprÃ¨s remap)\n",
    "        # On vÃ©rifie l'atteignabilitÃ© des paramÃ¨tres *utilisÃ©s* dans ce graph.\n",
    "        params_for_reach = list(iter_params_by_semantic.values())\n",
    "        reach = check_reachability(target, params_for_reach)\n",
    "        unreachable = [pname for pname, ok in reach.items() if not ok]\n",
    "        if unreachable:\n",
    "            print(f\"âš ï¸ Params non atteignables depuis target: {unreachable[:6]}{'...' if len(unreachable)>6 else ''}\")\n",
    "\n",
    "        all_targets.append(target)\n",
    "        all_feedbacks.append(run.feedback)\n",
    "\n",
    "    if not all_targets:\n",
    "        return {}, optimizer\n",
    "\n",
    "    # CrÃ©er l'optimiseur au 1er tour\n",
    "    if optimizer is None:\n",
    "        optimizer = OptoPrimeV2(\n",
    "            iter_params_by_semantic.values(),\n",
    "            llm=llm_client,\n",
    "            memory_size=memory_size,\n",
    "            log=True,\n",
    "            optimizer_prompt_symbol_set=OptimizerPromptSymbolSetJSON(),\n",
    "            objective=objective,\n",
    "        )\n",
    "        # initialiser mapping pour la suite\n",
    "        opt_params_by_semantic = {\n",
    "            (p.name.split(\":\")[0].split(\"/\")[-1]): p for p in getattr(optimizer, \"parameters\", []) or []\n",
    "        }\n",
    "\n",
    "    # Ajouter des descriptions si fournies\n",
    "    if param_descriptions:\n",
    "        _ensure_param_descriptions_on_optimizer(optimizer, list(iter_params_by_semantic.values()), param_descriptions)\n",
    "\n",
    "    # Batchify et optimiser\n",
    "    batched_target = batchify(*all_targets).data\n",
    "    batched_feedback = batchify(*all_feedbacks).data\n",
    "\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(batched_target, batched_feedback)\n",
    "    optimizer.step(verbose=False)\n",
    "\n",
    "    updates: Dict[str, str] = {}\n",
    "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
    "        full_name = getattr(p, \"name\", \"\") or \"\"\n",
    "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
    "        updates[semantic_name] = getattr(p, \"data\", \"\")\n",
    "\n",
    "    return updates, optimizer\n",
    "\n",
    "\n",
    "\n",
    "def apply_updates(\n",
    "    updates: Mapping[str, str],\n",
    "    *,\n",
    "    param_specs: Sequence[ParamSpec],\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Applique un dict d'updates (sortie OptoPrime) sur les ParamSpec.\n",
    "\n",
    "    Les ParamSpec dont `apply_update` est None sont ignorÃ©s.\n",
    "\n",
    "    Args:\n",
    "        updates: mapping semantic_name -> new_value.\n",
    "        param_specs: specs connus.\n",
    "\n",
    "    Returns:\n",
    "        dict \"appliquÃ©\" : semantic_name -> \"ok\"/\"skipped\"/\"error:...\"\n",
    "    \"\"\"\n",
    "    specs_by_name = {s.name: s for s in param_specs}\n",
    "    out: Dict[str, str] = {}\n",
    "\n",
    "    for semantic, new_val in updates.items():\n",
    "        spec = specs_by_name.get(semantic)\n",
    "        if spec is None:\n",
    "            out[semantic] = \"skipped: unknown_param\"\n",
    "            continue\n",
    "        if spec.apply_update is None:\n",
    "            out[semantic] = \"skipped: no_apply_update\"\n",
    "            continue\n",
    "        try:\n",
    "            spec.apply_update(str(new_val))\n",
    "            out[semantic] = \"ok\"\n",
    "        except Exception as e:\n",
    "            out[semantic] = f\"error: {type(e).__name__}: {e}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE8g-aJyxBOp"
   },
   "outputs": [],
   "source": [
    "# --- Trace/OptoPrime optimisation (non-intrusive) ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if str(Path('.').resolve()) not in sys.path:\n",
    "    sys.path.append(str(Path('.').resolve()))\n",
    "\n",
    "import trace_optimize_runtime as tor\n",
    "\n",
    "exporter, processor, status = tor.try_attach_inmemory_span_exporter()\n",
    "print('OTEL in-memory exporter status:', status)\n",
    "\n",
    "store = tor.TextOverrideStore()\n",
    "\n",
    "plan_prompt = tor.wrap_prompt_builder_with_addendum(\n",
    "    plan_prompt, store=store, addendum_key='planner_addendum'\n",
    ")\n",
    "executor_prompt = tor.wrap_prompt_builder_with_addendum(\n",
    "    executor_prompt, store=store, addendum_key='executor_addendum'\n",
    ")\n",
    "\n",
    "planner_addendum = store.as_param_spec(\n",
    "    name='planner_addendum',\n",
    "    attach_to=tor.SpanMatcher(name_contains=('planner',)),\n",
    "    trainable=True,\n",
    "    description='Append-only instructions added to the planner prompt.'\n",
    ")\n",
    "executor_addendum = store.as_param_spec(\n",
    "    name='executor_addendum',\n",
    "    attach_to=tor.SpanMatcher(name_contains=('executor',)),\n",
    "    trainable=True,\n",
    "    description='Append-only instructions added to the executor prompt.'\n",
    ")\n",
    "\n",
    "# Parametres RAG optimisables\n",
    "class RAGConfigStore:\n",
    "    def __init__(self):\n",
    "        self.simple_k = 5\n",
    "        self.parent_k = 3\n",
    "        self.child_chunk_size = 400\n",
    "        self.parent_chunk_size = 2000\n",
    "        self.use_parent_threshold = 15\n",
    "    \n",
    "    def get_simple_k(self):\n",
    "        return self.simple_k\n",
    "    \n",
    "    def set_simple_k(self, val):\n",
    "        try:\n",
    "            self.simple_k = max(1, min(20, int(val)))\n",
    "            simple_retriever.search_kwargs = {\"k\": self.simple_k}\n",
    "            print(f\"Updated simple_k to {self.simple_k}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update simple_k: {e}\")\n",
    "    \n",
    "    def get_parent_k(self):\n",
    "        return self.parent_k\n",
    "    \n",
    "    def set_parent_k(self, val):\n",
    "        try:\n",
    "            self.parent_k = max(1, min(10, int(val)))\n",
    "            print(f\"Updated parent_k to {self.parent_k}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update parent_k: {e}\")\n",
    "    \n",
    "    def get_use_parent_threshold(self):\n",
    "        return self.use_parent_threshold\n",
    "    \n",
    "    def set_use_parent_threshold(self, val):\n",
    "        try:\n",
    "            self.use_parent_threshold = max(5, min(50, int(val)))\n",
    "            print(f\"Updated use_parent_threshold to {self.use_parent_threshold}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update use_parent_threshold: {e}\")\n",
    "\n",
    "rag_config = RAGConfigStore()\n",
    "\n",
    "rag_simple_k = tor.ParamSpec(\n",
    "    name='rag_simple_k',\n",
    "    attach_to=tor.SpanMatcher(name_contains=('cortex',)),\n",
    "    trainable=True,\n",
    "    description='Nombre de chunks retournes par simple_retriever (1-20). Plus eleve = plus de contexte mais plus de bruit.',\n",
    "    get_value=lambda: str(rag_config.get_simple_k()),\n",
    "    apply_update=rag_config.set_simple_k\n",
    ")\n",
    "\n",
    "rag_parent_k = tor.ParamSpec(\n",
    "    name='rag_parent_k',\n",
    "    attach_to=tor.SpanMatcher(name_contains=('cortex',)),\n",
    "    trainable=True,\n",
    "    description='Nombre de chunks parents retournes par ParentDocumentRetriever (1-10). Parents = contexte large.',\n",
    "    get_value=lambda: str(rag_config.get_parent_k()),\n",
    "    apply_update=rag_config.set_parent_k\n",
    ")\n",
    "\n",
    "rag_threshold = tor.ParamSpec(\n",
    "    name='rag_use_parent_threshold',\n",
    "    attach_to=tor.SpanMatcher(name_contains=('cortex',)),\n",
    "    trainable=True,\n",
    "    description='Seuil de longueur de requete (en mots) pour activer ParentDocumentRetriever au lieu de simple (5-50).',\n",
    "    get_value=lambda: str(rag_config.get_use_parent_threshold()),\n",
    "    apply_update=rag_config.set_use_parent_threshold\n",
    ")\n",
    "\n",
    "CODE_TARGETS = []\n",
    "try:\n",
    "    CODE_TARGETS += [\n",
    "        tor.CodeTarget(\n",
    "            key='planner_node',\n",
    "            get_callable=lambda: planner_node,\n",
    "            attach_to=tor.SpanMatcher(name_contains=('planner',)),\n",
    "            description='LangGraph node that produces/updates the plan JSON.'\n",
    "        ),\n",
    "        tor.CodeTarget(\n",
    "            key='executor_node',\n",
    "            get_callable=lambda: executor_node,\n",
    "            attach_to=tor.SpanMatcher(name_contains=('executor',)),\n",
    "            description='LangGraph node that executes one plan step.'\n",
    "        ),\n",
    "        tor.CodeTarget(\n",
    "            key='synthesizer_node',\n",
    "            get_callable=lambda: synthesizer_node,\n",
    "            attach_to=tor.SpanMatcher(name_contains=('synthesizer',)),\n",
    "            description='Final synthesis / answer node.'\n",
    "        ),\n",
    "        tor.CodeTarget(\n",
    "            key='cortex_agents_research_node',\n",
    "            get_callable=lambda: cortex_agents_research_node,\n",
    "            attach_to=tor.SpanMatcher(name_contains=('cortex',)),\n",
    "            description='RAG + SQL research node with dual tool selection (local_rag_tool, duckdb_sql_tool).'\n",
    "        ),\n",
    "    ]\n",
    "except Exception as e:\n",
    "    print('Could not import code targets from helper:', e)\n",
    "\n",
    "code_param_specs = tor.build_code_param_specs(CODE_TARGETS)\n",
    "\n",
    "PARAM_SPECS = [\n",
    "    planner_addendum, \n",
    "    executor_addendum,\n",
    "    rag_simple_k,\n",
    "    rag_parent_k,\n",
    "    rag_threshold\n",
    "] + code_param_specs\n",
    "\n",
    "PARAM_DESC = tor.param_descriptions_from_specs(PARAM_SPECS)\n",
    "\n",
    "METRIC_KEYS = [\n",
    "    'Groundedness',\n",
    "    'Answer Relevance',\n",
    "    'Context Relevance',\n",
    "    'Logical Consistency',\n",
    "    'Execution Efficiency',\n",
    "    'Plan Adherence',\n",
    "    'Plan Quality',\n",
    "]\n",
    "METRIC_WEIGHTS = {\n",
    "    'Groundedness': 1.5,\n",
    "    'Answer Relevance': 1.5,\n",
    "    'Context Relevance': 1.2,\n",
    "    'Logical Consistency': 1.0,\n",
    "    'Execution Efficiency': 0.8,\n",
    "    'Plan Adherence': 1.0,\n",
    "    'Plan Quality': 1.0,\n",
    "}\n",
    "\n",
    "OBJECTIVE = \"\"\"You are optimizing a multi-agent LangGraph workflow with RAG and SQL capabilities.\n",
    "\n",
    "Goal:\n",
    "- Maximize eval.score (0..1), which aggregates eval.<metrics>.\n",
    "\n",
    "Key improvements to consider:\n",
    "- RAG parameters: adjust rag_simple_k (chunks retrieved), rag_parent_k (parent chunks), rag_use_parent_threshold (when to use hierarchical retrieval)\n",
    "- Prompt addendums: refine planner_addendum and executor_addendum to improve planning and execution\n",
    "- Code optimization: improve node logic if needed\n",
    "\n",
    "Constraints:\n",
    "- Keep function signatures unchanged.\n",
    "- Prefer minimal diffs.\n",
    "- Do not remove safety constraints.\n",
    "- If you edit code, keep it readable and deterministic.\n",
    "- RAG parameters must stay within valid ranges (k: 1-20, parent_k: 1-10, threshold: 5-50).\n",
    "\"\"\"\n",
    "\n",
    "def run_query_collect(query: str):\n",
    "    if exporter is not None and hasattr(exporter, 'clear'):\n",
    "        exporter.clear()\n",
    "\n",
    "    with tru_recorder as recording:\n",
    "        out = graph.invoke({'messages': [('user', query)]}, config=thread_config)\n",
    "\n",
    "    otlp = None\n",
    "    if exporter is not None:\n",
    "        otlp = tor.flush_inmemory_exporter_to_otlp(\n",
    "            exporter,\n",
    "            service_name='l6',\n",
    "            scope_name='trulens_otel',\n",
    "            clear=True\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        recs, fbs = session.get_records_and_feedback(app_ids=[tru_recorder.app_id])\n",
    "    except Exception:\n",
    "        recs, fbs = session.get_records_and_feedback()\n",
    "\n",
    "    row = tor.select_latest_item(recs)\n",
    "    row_map = tor.extract_mapping(row)\n",
    "\n",
    "    if (otlp is None) or (len(list(tor.otlp_iter_spans(otlp))) == 0):\n",
    "        record_json = tor.extract_trulens_record_json(row)\n",
    "        if record_json is None:\n",
    "            raise RuntimeError('No OTEL spans and no record_json found. Cannot build trace.')\n",
    "        otlp = tor.trulens_record_to_otlp(record_json, service_name='l6', scope_name='trulens_record')\n",
    "\n",
    "    metrics = tor.extract_metrics_from_mapping(row_map, metric_keys=METRIC_KEYS, default_metric=0.5)\n",
    "    score = tor.compute_score(metrics, weights=METRIC_WEIGHTS)\n",
    "\n",
    "    reasons = ''\n",
    "    for k in METRIC_KEYS:\n",
    "        for rk in (f'{k}_reasons', f'{k}.reasons', f'{k}_reason', f'{k}.reason'):\n",
    "            if rk in row_map and row_map[rk]:\n",
    "                reasons += f\"\\n[{k}] {row_map[rk]}\"\n",
    "\n",
    "    feedback = tor.render_feedback_text(\n",
    "        score=score,\n",
    "        metrics=metrics,\n",
    "        reasons=reasons,\n",
    "        extra={'query': query}\n",
    "    )\n",
    "\n",
    "    otlp_ready = tor.prepare_otlp_for_optimizer(\n",
    "        otlp,\n",
    "        param_specs=PARAM_SPECS,\n",
    "        score=score,\n",
    "        metrics=metrics,\n",
    "        reasons=reasons,\n",
    "        evaluator_parent_matcher=tor.SpanMatcher(name_contains=('synthesizer',)),\n",
    "        service_name='l6',\n",
    "        scope_name='trace_opt',\n",
    "    )\n",
    "\n",
    "    return tor.RunResult(\n",
    "        otlp=otlp_ready,\n",
    "        score=score,\n",
    "        metrics=metrics,\n",
    "        feedback=feedback,\n",
    "        meta={'query': query}\n",
    "    )\n",
    "\n",
    "QUERIES = [\n",
    "    'Quels sont les principaux algorithmes de Multi-Agent RL mentionnes dans les documents?',\n",
    "    'Combien de documents traitent de cybersecurite?',\n",
    "    'Resumes les methodes de mesure de pression arterielle avec contexte detaille',\n",
    "]\n",
    "N_ITER = 3\n",
    "optimizer = None\n",
    "\n",
    "from opto.utils.llm import LLM\n",
    "LLM_CLIENT = LLM()\n",
    "\n",
    "print(\"Configuration initiale RAG:\")\n",
    "print(f\"  simple_k: {rag_config.simple_k}\")\n",
    "print(f\"  parent_k: {rag_config.parent_k}\")\n",
    "print(f\"  use_parent_threshold: {rag_config.use_parent_threshold}\")\n",
    "\n",
    "for it in range(N_ITER):\n",
    "    runs = [run_query_collect(q) for q in QUERIES]\n",
    "    print(f'\\n=== Iteration {it} ===')\n",
    "    print('Scores:', [round(r.score, 3) for r in runs])\n",
    "    print('Avg score:', round(sum(r.score for r in runs) / len(runs), 3))\n",
    "\n",
    "    updates, optimizer = tor.optimize_iteration(\n",
    "        runs,\n",
    "        optimizer=optimizer,\n",
    "        llm_client=LLM_CLIENT,\n",
    "        objective=OBJECTIVE,\n",
    "        param_name_substrings=('__code_', 'planner_addendum', 'executor_addendum', 'rag_'),\n",
    "        memory_size=12,\n",
    "        verbose_graph=False,\n",
    "        param_descriptions=PARAM_DESC,\n",
    "        prefer_target_name_contains='evaluator',\n",
    "    )\n",
    "\n",
    "    applied = tor.apply_updates(updates, param_specs=PARAM_SPECS)\n",
    "    print('Applied:', {k:v for k,v in applied.items() if v != 'skipped: unknown_param'})\n",
    "    \n",
    "    print(\"\\nConfiguration RAG apres optimisation:\")\n",
    "    print(f\"  simple_k: {rag_config.simple_k}\")\n",
    "    print(f\"  parent_k: {rag_config.parent_k}\")\n",
    "    print(f\"  use_parent_threshold: {rag_config.use_parent_threshold}\")\n",
    "\n",
    "print('\\n=== Resultats finaux ===')\n",
    "print('Configuration RAG optimisee:')\n",
    "print(f'  simple_k: {rag_config.simple_k}')\n",
    "print(f'  parent_k: {rag_config.parent_k}')\n",
    "print(f'  use_parent_threshold: {rag_config.use_parent_threshold}')\n",
    "print('\\nAddendums optimises:')\n",
    "print('planner_addendum:\\n', store.get('planner_addendum'))\n",
    "print('executor_addendum:\\n', store.get('executor_addendum'))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
