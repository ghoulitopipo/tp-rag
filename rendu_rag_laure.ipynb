{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61a3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pas sur Google Colab, ces commandes ne seront pas exécutées.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Vérifie si le code est exécuté sur Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Commandes à exécuter uniquement sur Google Colab\n",
    "    !git clone https://github.com/GITHUB_ACCOUNT/tp-rag-student-version.git\n",
    "    %cd tp-rag\n",
    "    !pip install -r requirements.txt\n",
    "else:\n",
    "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
    "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9d2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances nécessaires (avec versions compatibles)\n",
    "!pip install -q --upgrade langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface chromadb sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0a02b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : cuda\n",
      "GPU détecté : NVIDIA GeForce RTX 3060 Ti\n",
      "Mémoire GPU disponible : 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device utilisé : {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU détecté : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Mémoire GPU disponible : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddae5e4",
   "metadata": {},
   "source": [
    "# TP - Retrieval Augmented Generation\n",
    "\n",
    "## Étape 1 : Indexation des documents\n",
    "\n",
    "## Exercice 1 : Indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bafa5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de fichiers trouvés : 65\n",
      "Nombre total de documents chargés : 600\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "data_root = Path('data') if 'COLAB_GPU' not in os.environ else Path('tp-rag-student-version') / 'data'\n",
    "\n",
    "text_exts = {'.txt', '.tex', '.bib'}\n",
    "pdf_ext = '.pdf'\n",
    "\n",
    "documents = []\n",
    "data_files = []\n",
    "\n",
    "if data_root.exists():\n",
    "    for p in sorted(data_root.rglob('*')):\n",
    "        if p.is_file() and p.suffix.lower() in text_exts.union({pdf_ext}):\n",
    "            data_files.append(str(p))\n",
    "else:\n",
    "    print(f\"Le chemin de données {data_root} n'existe pas\")\n",
    "\n",
    "for fp in data_files:\n",
    "    p = Path(fp)\n",
    "    if p.suffix.lower() in text_exts:\n",
    "        try:\n",
    "            loader = TextLoader(str(p), encoding='utf-8')\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur en chargeant {p}: {e}\")\n",
    "    elif p.suffix.lower() == pdf_ext:\n",
    "        try:\n",
    "            pdf_loader = PyPDFLoader(str(p))\n",
    "            docs = pdf_loader.load()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur en chargeant {p}: {e}\")\n",
    "\n",
    "print(f\"Nombre total de fichiers trouvés : {len(data_files)}\")\n",
    "print(f\"Nombre total de documents chargés : {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd56a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de chunks créés : 9804\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Nombre de chunks créés : {len(splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1b52b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lilia\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle d'embeddings chargé : intfloat/multilingual-e5-base\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"Modèle d'embeddings chargé : intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc42f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents indexés : 11489\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"Nombre de documents indexés : {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10210e28",
   "metadata": {},
   "source": [
    "## Exercice 2 : Interrogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52b4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, k=5):\n",
    "    \"\"\"\n",
    "    Interroge la base vectorielle et retourne les documents les plus pertinents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): La requête de recherche\n",
    "        k (int): Le nombre de documents à retourner (défaut: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste de tuples (document, score) avec les documents pertinents et leurs scores\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_search_results(results):\n",
    "    \"\"\"\n",
    "    Affiche les résultats de recherche de manière formatée.\n",
    "    \n",
    "    Args:\n",
    "        results (list): Liste de tuples (document, score)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Nombre de résultats : {len(results)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"Résultat {i} - Score de similarité : {score:.4f}\")\n",
    "        print(f\"Source : {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Contenu : {doc.page_content[:300]}...\")\n",
    "        print(f\"{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9906c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Nombre de résultats : 3\n",
      "================================================================================\n",
      "\n",
      "Résultat 1 - Score de similarité : 0.2646\n",
      "Source : data\\autres_articles\\2412.18607v1.pdf\n",
      "Contenu : marks demonstrates the effectiveness of the proposed Driv-\n",
      "ingGPT on action-conditioned video generation and end-\n",
      "to-end planning. DrivingGPT clearly shows the viability of\n",
      "unified learning of world modeling and planning with a sin-\n",
      "gle model, setting a stepping stone for the future exploration\n",
      "of d...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Résultat 2 - Score de similarité : 0.2835\n",
      "Source : data\\autres_articles\\2412.18607v1.pdf\n",
      "Contenu : DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal\n",
      "Autoregressive Transformers\n",
      "Supplementary Material\n",
      "A. Refining Video Generation with SVD De-\n",
      "coder.\n",
      "Since independently decoding each frame-token to pixel\n",
      "space leads to temporally inconsistent video outputs, we\n",
      "further employ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Résultat 3 - Score de similarité : 0.2862\n",
      "Source : data\\autres_articles\\2412.18607v1.pdf\n",
      "Contenu : driving language based on interleaved image and action\n",
      "tokens, and develop DrivingGPT to learn joint world mod-\n",
      "eling and planning through standard next-token prediction.\n",
      "Our DrivingGPT demonstrates strong performance in both\n",
      "action-conditioned video generation and end-to-end plan-\n",
      "ning, outperformi...\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Explain DrivingGPT.\"\n",
    "results1 = search_documents(query1, k=3)\n",
    "display_search_results(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559f2eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Nombre de résultats : 3\n",
      "================================================================================\n",
      "\n",
      "Résultat 1 - Score de similarité : 0.2764\n",
      "Source : data\\patents\\APPARATUS FOR THE MEASUREMENT OF ATRIAL PRESSURE.txt\n",
      "Contenu : EP\t0615422\tB1\t2000-01-19\ten\tTITLE\t1\tAPPARATUS FOR THE MEASUREMENT OF ATRIAL PRESSURE...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Résultat 2 - Score de similarité : 0.2764\n",
      "Source : data/patents/APPARATUS FOR THE MEASUREMENT OF ATRIAL PRESSURE.txt\n",
      "Contenu : EP\t0615422\tB1\t2000-01-19\ten\tTITLE\t1\tAPPARATUS FOR THE MEASUREMENT OF ATRIAL PRESSURE...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Résultat 3 - Score de similarité : 0.2945\n",
      "Source : data\\arxiv\\A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.pdf\n",
      "Contenu : still remain popular in clinical settings.\n",
      "2) Oscillometric: This method is currently the most popular\n",
      "technology for automated BP measurement devices [6]. Using\n",
      "a pressure sensor, it measures the pulsatile BP in the artery\n",
      "during cuff inflation and deflation [5]. In this technology, the\n",
      "transducer ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"Comment mesurer la pression artérielle ?\"\n",
    "results2 = search_documents(query2, k=3)\n",
    "display_search_results(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740817d",
   "metadata": {},
   "source": [
    "# Étape 2 - RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845bfa2",
   "metadata": {},
   "source": [
    "## Exercice 3 : Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3e9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables du template : ['context', 'question']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "rag_prompt_template = \"\"\"Tu es un assistant intelligent et utile qui répond aux questions en te basant sur le contexte fourni.\n",
    "\n",
    "Utilise UNIQUEMENT les informations du contexte ci-dessous pour répondre à la question. Si le contexte ne contient pas l'information nécessaire pour répondre, dis-le clairement.\n",
    "\n",
    "Contexte :\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Instructions :\n",
    "- Réponds de manière claire et concise\n",
    "- Cite les sources si pertinent\n",
    "- Si l'information n'est pas dans le contexte, indique-le clairement\n",
    "- Utilise un ton professionnel mais accessible\n",
    "\n",
    "Réponse :\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "print(f\"\\nVariables du template : {prompt.input_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b53a8",
   "metadata": {},
   "source": [
    "## Exercice 4 : Chaîne RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0259f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06bd3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Formatte les documents récupérés pour le contexte.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def rag_chain(question, vectorstore, prompt_template, llm, k=3):\n",
    "    \"\"\"\n",
    "    Chaîne RAG complète : récupération + génération de réponse.\n",
    "    \n",
    "    Args:\n",
    "        question (str): La question de l'utilisateur\n",
    "        vectorstore: Le vector store contenant les documents\n",
    "        prompt_template: Le template de prompt à utiliser\n",
    "        llm: Le modèle de langage\n",
    "        k (int): Nombre de documents à récupérer\n",
    "    \n",
    "    Returns:\n",
    "        str: La réponse générée\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    response = chain.invoke(question)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa67c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Qu'est-ce que le Multi-Agent Reinforcement Learning et quelles sont ses applications ?\n",
      "\n",
      "================================================================================\n",
      "Réponse : Le **Multi-Agent Reinforcement Learning (MARL)** est une technique d'intelligence artificielle (IA) qui permet à plusieurs agents (robots, programmes, etc.) d'apprendre à collaborer ou compétiter en s'adaptant aux interactions mutuelles dans un environnement. Il vise à optimiser les stratégies collectives en tenant compte des comportements des autres agents.  \n",
      "\n",
      "**Applications** :  \n",
      "Le contexte mentionne que le MARL est utilisé dans des scénarios d'apprentissage coopératif, notamment pour des tâches nécessitant la collaboration entre agents (ex. : gestion de ressources, contrôle de systèmes distribués). Des méthodes comme **LOLA** (Learning with Opponent-Learning Awareness) illustrent comment les agents peuvent développer des politiques coopératives en tenant compte des actions des autres. Cependant, le contexte ne fournit pas d'exemples concrets d'applications industrielles ou spécifiques.  \n",
      "\n",
      "**Limites et enjeux** :  \n",
      "Le MARL fait face à des défis comme la **scalabilité**, la **non-stationnarité** (changement des comportements des agents) et la **fiabilité** (sécurité et éthique). Le texte souligne l'importance de rendre le MARL \"trustworthy\" (confiant) pour des applications pratiques, notamment dans les interactions humain-machine.  \n",
      "\n",
      "*Source : Contexte fourni (Zhou et al., 2023)*.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "question1 = \"Qu'est-ce que le Multi-Agent Reinforcement Learning et quelles sont ses applications ?\"\n",
    "\n",
    "print(\"Question :\", question1)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "response1 = rag_chain(question1, vectorstore, prompt, llm, k=3)\n",
    "print(\"Réponse :\", response1)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b314cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Comment mesurer la pression artérielle et quelles sont les sources d'erreur potentielles ?\n",
      "\n",
      "================================================================================\n",
      "Réponse : Pour mesurer la pression artérielle, les méthodes courantes incluent :  \n",
      "- **Mesure en cabinet** (avec sphygmomanomètre manuel ou dispositif automatique).  \n",
      "- **Mesure hors cabinet** (monitorage ambulatoire ou mesure 24 heures).  \n",
      "- **Auto-mesure** (dispositifs portables utilisés à domicile).  \n",
      "\n",
      "**Sources d'erreur potentielles** :  \n",
      "1. **Technique inadéquate** (position incorrecte, inflation insuffisante du ballonnet).  \n",
      "2. **Calibration incorrecte** des dispositifs (défaillances ou manque de maintenance).  \n",
      "3. **Facteurs patient** (mouvement, anxiété, caféine, ou prise de médicaments).  \n",
      "4. **Environnement** (bruit, température, ou interférences électromagnétiques).  \n",
      "5. **Erreurs de lecture** (interprétation erronée des données par les patients).  \n",
      "\n",
      "Ces points sont abordés dans les guides de l'European Society of Hypertension ([43]) et dans une revue systématique sur les erreurs des mesures automatiques ([44]). La difficulté des auto-mesures est également mentionnée ([42]).\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "question2 = \"Comment mesurer la pression artérielle et quelles sont les sources d'erreur potentielles ?\"\n",
    "\n",
    "print(\"Question :\", question2)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "response2 = rag_chain(question2, vectorstore, prompt, llm, k=3)\n",
    "print(\"Réponse :\", response2)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6daa4f7",
   "metadata": {},
   "source": [
    "## Exercice 5 : Mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22ed561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Étant donné un historique de conversation et la dernière question de l'utilisateur \n",
    "qui pourrait faire référence au contexte de l'historique de conversation, formulez une question autonome \n",
    "qui peut être comprise sans l'historique de conversation. Ne répondez PAS à la question, \n",
    "reformulez-la simplement si nécessaire, sinon retournez-la telle quelle.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_system_prompt = \"\"\"Tu es un assistant intelligent et utile qui répond aux questions en te basant sur le contexte fourni.\n",
    "\n",
    "Utilise UNIQUEMENT les informations du contexte ci-dessous pour répondre à la question. \n",
    "Si le contexte ne contient pas l'information nécessaire pour répondre, dis-le clairement.\n",
    "\n",
    "Contexte :\n",
    "{context}\n",
    "\n",
    "Instructions :\n",
    "- Réponds de manière claire et concise\n",
    "- Cite les sources si pertinent\n",
    "- Si l'information n'est pas dans le contexte, indique-le clairement\n",
    "- Utilise un ton professionnel mais accessible\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04f67024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag_chain(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    Crée une chaîne RAG avec gestion de l'historique de conversation.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Le vector store contenant les documents\n",
    "        llm: Le modèle de langage\n",
    "    \n",
    "    Returns:\n",
    "        Une chaîne RAG conversationnelle\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    \n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    \n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "conversational_rag = create_conversational_rag_chain(vectorstore, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e34e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 : Qu'est-ce que le Multi-Agent Reinforcement Learning ?\n",
      "Réponse 1 : Le **Multi-Agent Reinforcement Learning (MARL)** est un domaine de la science des données et de l'intelligence artificielle qui étudie comment plusieurs agents (robots, programmes, etc.) apprennent à prendre des décisions collaboratives ou compétitives dans un environnement partagé. Les agents ajustent leurs stratégies en fonction des interactions avec leur environnement et les autres agents, en maximisant un critère de récompense collectif ou individuel.\n",
      "\n",
      "### Points clés du contexte :\n",
      "1. **Défis et méthodes** :  \n",
      "   - Des approches comme **LOLA** (Learning with Opponent-Learning Awareness) intègrent l'impact des politiques d'un agent sur les mises à jour des paramètres des autres, favorisant la coopération (Foerster et al., 2018).  \n",
      "   - Des attaques adversariales sont utilisées pour renforcer la robustesse des politiques face aux perturbations (ex. : perturbations des observations).  \n",
      "\n",
      "2. **Applications** :  \n",
      "   - Utilisation en recommandation multi-module (ex. : système de recommandation collaboratif sans communication) (He et al., 2020).  \n",
      "\n",
      "3. **Enjeux** :  \n",
      "   - La coordination entre agents, la stabilité des équilibres de Nash, et la résilience face aux attaques sont des défis majeurs (Kapoor, 2018).  \n",
      "\n",
      "Pour en savoir plus, consultez les références citées (Foerster et al., 2018 ; He et al., 2020 ; Kapoor, 2018).\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 2 : Quelles sont ses principales applications ?\n",
      "Réponse 2 : Le **Multi-Agent Reinforcement Learning (MARL)** trouve des applications dans plusieurs domaines, notamment :  \n",
      "\n",
      "1. **Contrôle du trafic** :  \n",
      "   - Utilisation de l'apprentissage par renforcement pour optimiser les signaux de circulation, en tenant compte des interactions entre les agents (véhicules, feux tricolores) (source : *Reinforcement Learning for Traffic Signal Control Optimization*).  \n",
      "\n",
      "2. **Gestion de la demande énergétique** :  \n",
      "   - Applications à la réponse à la demande rapide des charges résidentielles, permettant d'ajuster la consommation d'énergie en temps réel (source : *Multi-Agent Reinforcement Learning for Fast-Timescale Demand Response*).  \n",
      "\n",
      "3. **Planification de mouvement décentralisée** :  \n",
      "   - Développement de systèmes de planification de mouvement multi-robots ou multi-agents, sans communication directe entre les agents (source : *Learning a decentralized multi-arm motion planner*).  \n",
      "\n",
      "4. **Gestion des agents défaillants** :  \n",
      "   - Frameworks comme **Coach-assisted MARL** pour gérer les cas de crash d'agents, en assurant la continuité des interactions (source : *Coach-assisted multi-agent reinforcement learning framework for unexpected crashed agents*).  \n",
      "\n",
      "5. **Interaction humain-machine** :  \n",
      "   - Applications dans des scénarios de collaboration entre agents autonomes et humains, en intégrant des contraintes éthiques et de sécurité (source : *Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges*).  \n",
      "\n",
      "Ces applications illustrent la flexibilité de MARL pour des problèmes complexes impliquant des interactions dynamiques et des objectifs collectifs.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 3 : Parle-moi des méthodes de mesure de la pression artérielle\n",
      "Réponse 3 : Les méthodes de mesure de la pression artérielle se divisent en **invasives** et **non-invasives**, chacune avec des applications spécifiques et des avantages/désavantages. Voici une synthèse basée sur le contexte :\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Méthodes invasives**  \n",
      "Ces techniques impliquent l'insertion d'un capteur directement dans le système circulatoire. Elles sont principalement utilisées dans des contextes médicaux critiques.  \n",
      "- **Intravasculaire** :  \n",
      "  - Le capteur est inséré dans le vaisseau sanguin (par exemple, via un cathéter).  \n",
      "  - Utilisé pour surveiller la pression artérielle en temps réel dans des unités comme les **ICUs** (unités de soins intensifs) ou les **salles d'opération**.  \n",
      "  - Permet des mesures précises, mais comporte des risques de complications (infections, hémorragies).  \n",
      "- **Extra-vasculaire** :  \n",
      "  - Le capteur est placé en dehors du vaisseau, le long du cathéter.  \n",
      "  - Utilisé pour surveiller la pression artérielle ou veineuse, mais moins courant que l'intravasculaire.  \n",
      "\n",
      "**Sources** : [34] (Webster, 2009), [35] (Kumar et al., 2021).  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Méthodes non-invasives**  \n",
      "Ces techniques ne nécessitent pas d'intervention invasive et sont plus courantes dans les environnements cliniques et domestiques.  \n",
      "- **Oscillométrique** :  \n",
      "  - La méthode la plus courante, utilisant un **manomètre à ballonnet** (cuff) autour du bras.  \n",
      "  - Mesure la pression artérielle en détectant les oscillations du flux sanguin.  \n",
      "  - Utilisée dans les cabinets médicaux, les centres de santé, et à domicile.  \n",
      "- **Pulse Transit Time (PTT)** :  \n",
      "  - Mesure le temps de transit du pouls entre deux points (ex. : doigts ou poignets).  \n",
      "  - Permet une surveillance continue et portable, souvent combinée à des capteurs optiques ou électriques.  \n",
      "  - Mentionné comme une méthode prometteuse pour la surveillance non-invasive (source : [36]).  \n",
      "- **Électrocardiographie (ECG)** :  \n",
      "  - Utilisée pour estimer la pression artérielle en analysant les signaux cardiaques, bien que moins précise que les méthodes directes.  \n",
      "\n",
      "**Sources** : [36] (Peter et al., 2014), [37] (Lim et al., 2022).  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Comparaison et contexte**  \n",
      "- **Invasive** : Précise, mais limitée aux environnements hospitaliers.  \n",
      "- **Non-invasive** : Plus accessible, mais moins précise.  \n",
      "- **Applications** :  \n",
      "  - Les méthodes invasives sont essentielles pour les patients critiques (ex. : chirurgie, réanimation).  \n",
      "  - Les méthodes non-invasives sont utilisées pour le suivi quotidien ou à domicile.  \n",
      "\n",
      "**Sources** : [34] (Webster, 2009), [35] (Kumar et al., 2021), [36] (Peter et al., 2014).  \n",
      "\n",
      "---\n",
      "\n",
      "Si vous souhaitez des détails supplémentaires sur une méthode spécifique (ex. : PTT, oscillométrique), n'hésitez pas à demander !\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question1 = \"Qu'est-ce que le Multi-Agent Reinforcement Learning ?\"\n",
    "response1 = conversational_rag.invoke({\n",
    "    \"input\": question1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(\"Question 1 :\", question1)\n",
    "print(\"Réponse 1 :\", response1[\"answer\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=response1[\"answer\"])\n",
    "])\n",
    "\n",
    "question2 = \"Quelles sont ses principales applications ?\"\n",
    "response2 = conversational_rag.invoke({\n",
    "    \"input\": question2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(\"Question 2 :\", question2)\n",
    "print(\"Réponse 2 :\", response2[\"answer\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=response2[\"answer\"])\n",
    "])\n",
    "\n",
    "question3 = \"Parle-moi des méthodes de mesure de la pression artérielle\"\n",
    "response3 = conversational_rag.invoke({\n",
    "    \"input\": question3,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(\"Question 3 :\", question3)\n",
    "print(\"Réponse 3 :\", response3[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8f24b",
   "metadata": {},
   "source": [
    "## Exercice 6 : Nouveaux Outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a62b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def get_document_by_title(title_keyword, data_files, documents):\n",
    "    \"\"\"\n",
    "    Recherche un document par mot-clé dans le titre.\n",
    "    \n",
    "    Args:\n",
    "        title_keyword (str): Mot-clé à rechercher dans le nom du fichier\n",
    "        data_files (list): Liste des chemins de fichiers\n",
    "        documents (list): Liste des documents chargés\n",
    "    \n",
    "    Returns:\n",
    "        list: Documents correspondants\n",
    "    \"\"\"\n",
    "    matching_docs = []\n",
    "    \n",
    "    for i, file_path in enumerate(data_files):\n",
    "        if title_keyword.lower() in file_path.lower():\n",
    "            doc_chunks = [doc for doc in documents if doc.metadata.get('source', '') == file_path]\n",
    "            matching_docs.extend(doc_chunks)\n",
    "    \n",
    "    return matching_docs\n",
    "\n",
    "\n",
    "def summarize_document(title_keyword, llm, data_files, documents):\n",
    "    \"\"\"\n",
    "    Résume un document complet identifié par un mot-clé.\n",
    "    \n",
    "    Args:\n",
    "        title_keyword (str): Mot-clé pour identifier le document\n",
    "        llm: Le modèle de langage\n",
    "        data_files (list): Liste des fichiers de données\n",
    "        documents (list): Liste des documents chargés\n",
    "    \n",
    "    Returns:\n",
    "        str: Résumé du document\n",
    "    \"\"\"\n",
    "    doc_chunks = get_document_by_title(title_keyword, data_files, documents)\n",
    "    \n",
    "    if not doc_chunks:\n",
    "        return f\"Aucun document trouvé contenant '{title_keyword}'\"\n",
    "    \n",
    "    source = doc_chunks[0].metadata.get('source', 'Unknown')\n",
    "    print(f\"Document trouvé : {source}\")\n",
    "    print(f\"Nombre de chunks : {len(doc_chunks)}\")\n",
    "    \n",
    "    summary_prompt = PromptTemplate(\n",
    "        template=\"\"\"Écris un résumé concis et structuré du texte suivant.\n",
    "        \n",
    "        Texte :\n",
    "        {text}\n",
    "        \n",
    "        Résumé structuré :\"\"\",\n",
    "        input_variables=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    chain = load_summarize_chain(\n",
    "        llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=summary_prompt,\n",
    "        combine_prompt=summary_prompt,\n",
    "        verbose=False\n",
    "    )\n",
    "    summary = chain.run(doc_chunks)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6e025b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document trouvé : data\\latex\\Multi-Agent Reinforcement Learning, Methods, Applications, Visionary Prospects, and Challenges.bib\n",
      "Nombre de chunks : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilia\\AppData\\Local\\Temp\\ipykernel_29556\\4156399873.py:65: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  summary = chain.run(doc_chunks)\n",
      "C:\\Users\\lilia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\language_models\\base.py:328: UserWarning: Using fallback GPT-2 tokenizer for token counting. Token counts may be inaccurate for non-GPT-2 models. For accurate counts, use a model-specific method if available.\n",
      "  return len(self.get_token_ids(text))\n",
      "C:\\Users\\lilia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lilia\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RÉSUMÉ DU DOCUMENT\n",
      "================================================================================\n",
      "**Résumé structuré :**  \n",
      "\n",
      "### **1. Applications médicales et scientifiques**  \n",
      "- **Diagnostic médical** : Utilisation de RL pour améliorer la précision des diagnostics (cancer du sein, interprétation des symptômes) et optimiser les traitements via des systèmes d’assistance.  \n",
      "- **Recherche scientifique** : Applications en physique (turbulence, contrôle de plasmas tokamak) et gestion de systèmes complexes (fusion, énergétique).  \n",
      "\n",
      "### **2. Algorithmes et méthodes RL**  \n",
      "- **Techniques clés** : A3C (optimisation asynchrone), MAAC (multi-agents avec attention), et frameworks en temps réel pour gérer des données dynamiques.  \n",
      "- **Systèmes multi-agents** : Coordination d’agents dans des environnements complexes (ex. contrôle de plasmas, gestion de trafic).  \n",
      "\n",
      "### **3. Défis des systèmes HCPS (humain-centrés)**  \n",
      "- **Non-stationnarité** : Les comportements humains perturbent l’environnement, rendant les systèmes instables.  \n",
      "- **Diversité humaine** : Variations culturelles, géographiques et religieuses des comportements.  \n",
      "- **Hétérogénéité** : Composants physiques, informatiques et humains, avec des délais de décision variables.  \n",
      "- **Évolutivité** : Gestion de l’explosion de l’espace d’actions avec l’augmentation du nombre d’agents.  \n",
      "\n",
      "### **4. Perspectives et exigences**  \n",
      "- **Interdisciplinarité** : Croisement de RL avec médecine, physique, ingénierie pour résoudre des problèmes réels.  \n",
      "- **Optimisation** : Méthodes asynchrones et parallèles pour réduire les temps de calcul.  \n",
      "- **Trustworthiness** : Sécurité, robustesse, généralisation et éthique dans les décisions.  \n",
      "\n",
      "### **5. Conclusion**  \n",
      "Le RL et le MARL démontrent leur flexibilité et leur impact dans des domaines variés, en combinant avancées algorithmiques et applications concrètes. Pour être efficaces, ils doivent intégrer des facteurs humains et répondre à des exigences de confiance (sécurité, éthique, évolutivité).  \n",
      "\n",
      "**Mots-clés** : RL, MARL, HCPS, non-stationnarité, hétérogénéité, éthique, évolutivité, sécurité.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_document(\n",
    "    \"Multi-Agent Reinforcement Learning\", \n",
    "    llm, \n",
    "    data_files, \n",
    "    documents\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ DU DOCUMENT\")\n",
    "print(\"=\"*80)\n",
    "print(summary)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72476bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document trouvé : data\\arxiv\\A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.pdf\n",
      "Nombre de chunks : 22\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ DU DOCUMENT\n",
      "================================================================================\n",
      "**Résumé structuré :**  \n",
      "\n",
      "### **1. Contexte et importance**  \n",
      "- La mesure de la pression artérielle (PAP) est essentielle pour évaluer le risque cardiovasculaire, mais les dispositifs actuels (manuels, automatisés) présentent des biais techniques, physiologiques et environnementaux (âge, sexe, température, bruit), pouvant mener à des erreurs de diagnostic et de traitement.  \n",
      "\n",
      "### **2. Méthodes de mesure et standards**  \n",
      "- **Invasive** : Cathéter + transducteur (précision élevée, utilisée en soins intensifs).  \n",
      "- **Non invasive** :  \n",
      "  - **Cuff-based** : Korotkoff (manuelle) ou oscillométrique (automatisée).  \n",
      "  - **Cuff-less** : Tonométrie, PPG (en recherche, plus confortables).  \n",
      "- **Normes** : ISO 81060-2 (2018) et FDA (510(k)) pour valider la précision (erreur <5 mmHg, écart type <8 mmHg).  \n",
      "\n",
      "### **3. Facteurs influençant la mesure**  \n",
      "- **Physiologiques** : Fluctuations entre SBP et DBP (PMA = SBP + 2 × DBP / 3), variations circadiennes, différences inter-bras.  \n",
      "- **Environnementaux** : Température (15–25 °C), humidité (20–85 %), repos (30 min), position du bras.  \n",
      "- **Contextuels** : \"White-coat hypertension\" (hypertension en milieu clinique).  \n",
      "\n",
      "### **4. Solutions et innovations**  \n",
      "- **Technologies futures** :  \n",
      "  - **IA/ML** : Personnalisation des mesures et correction des biais.  \n",
      "  - **Capteurs intégrés** : Détectent les facteurs environnementaux (bruit, température).  \n",
      "  - **Mesures sans manchette** : Tonométrie, PWV, PPG (rapides et confortables).  \n",
      "\n",
      "### **5. Implications cliniques et résultats**  \n",
      "- **Risques** : PAP élevée et déclivité nocturne anormale augmentent les risques d’AVC, AIT et complications cardiovasculaires.  \n",
      "- **Gestion** : Réduction des antihypertenseurs améliore les résultats, surtout en milieu rural.  \n",
      "- **Outils d’analyse** : Logiciels comme **bp** (R) facilitent l’interprétation des données.  \n",
      "\n",
      "### **6. Conclusion**  \n",
      "- La précision des mesures de PAP dépend de la correction des biais techniques, physiologiques et contextuels.  \n",
      "- L’intégration de l’IA, des capteurs innovants et des normes strictes (ISO/FDA) est cruciale pour personnaliser les soins et réduire les erreurs cliniques.  \n",
      "\n",
      "**Mots-clés :** Mesure de PAP, biais de mesure, IA, normes ISO/FDA, personnalisation, capteurs environnementaux.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary2 = summarize_document(\n",
    "    \"Blood_Pressure\", \n",
    "    llm, \n",
    "    data_files, \n",
    "    documents\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ DU DOCUMENT\")\n",
    "print(\"=\"*80)\n",
    "print(summary2)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ac616",
   "metadata": {},
   "source": [
    "# Étape 3 - IHM\n",
    "\n",
    "## Exercice 7 : IHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "302851ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\lilia\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lilia\\AppData\\Local\\Temp\\ipykernel_29556\\162278108.py:148: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft(), title=\"Système RAG Complet\") as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f58e516c43c0de97c8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f58e516c43c0de97c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://f58e516c43c0de97c8.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "def advanced_chatbot(message, history, k_docs, show_sources):\n",
    "    \"\"\"\n",
    "    Traite une question avec le système RAG et retourne une réponse formatée.\n",
    "    \n",
    "    Args:\n",
    "        message: Question de l'utilisateur (string)\n",
    "        history: Historique de conversation Gradio (liste de dictionnaires)\n",
    "        k_docs: Nombre de documents à récupérer\n",
    "        show_sources: Si True, ajoute les sources à la réponse\n",
    "    \n",
    "    Returns:\n",
    "        Réponse formatée avec sources optionnelles (string)\n",
    "    \"\"\"\n",
    "    # Convertir l'historique Gradio (format dictionnaire) en format LangChain\n",
    "    chat_history = []\n",
    "    for msg_dict in history:\n",
    "        if isinstance(msg_dict, dict):\n",
    "            role = msg_dict.get(\"role\", \"\")\n",
    "            content = msg_dict.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                chat_history.append(HumanMessage(content=content))\n",
    "            elif role == \"assistant\":\n",
    "                chat_history.append(AIMessage(content=content))\n",
    "    \n",
    "    # Créer une chaîne RAG avec les paramètres personnalisés\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k_docs})\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    # Générer la réponse\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": str(message),  # S'assurer que c'est une chaîne\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    \n",
    "    answer = response[\"answer\"]\n",
    "    \n",
    "    # Ajouter les sources si demandé\n",
    "    if show_sources and \"context\" in response:\n",
    "        sources = {Path(doc.metadata.get(\"source\", \"Unknown\")).name \n",
    "                   for doc in response[\"context\"]}\n",
    "        \n",
    "        if sources:\n",
    "            answer += \"\\n\\n\" + \"─\" * 80 + \"\\n\"\n",
    "            answer += \"**Sources utilisées :**\\n\"\n",
    "            for i, source in enumerate(sorted(sources), 1):\n",
    "                answer += f\"  {i}. {source}\\n\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def search_interface(query, k):\n",
    "    \"\"\"\n",
    "    Recherche sémantique dans la base vectorielle.\n",
    "    \n",
    "    Args:\n",
    "        query: Requête de recherche\n",
    "        k: Nombre de résultats à retourner\n",
    "    \n",
    "    Returns:\n",
    "        Résultats formatés en Markdown\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Veuillez entrer une requête de recherche.\"\n",
    "    \n",
    "    results = search_documents(query, k=k)\n",
    "    \n",
    "    output = f\"# Résultats de recherche\\n\\n\"\n",
    "    output += f\"**{len(results)} documents trouvés pour :** *\\\"{query}\\\"*\\n\\n\"\n",
    "    output += \"═\" * 80 + \"\\n\\n\"\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        source_name = Path(doc.metadata.get('source', 'Unknown')).name\n",
    "        \n",
    "        output += f\"## Résultat {i}\\n\"\n",
    "        output += f\"**Score de similarité :** {score:.4f}  \\n\"\n",
    "        output += f\"**Source :** `{source_name}`\\n\\n\"\n",
    "        output += f\"**Extrait :**\\n```\\n{doc.page_content[:400]}...\\n```\\n\\n\"\n",
    "        output += \"─\" * 80 + \"\\n\\n\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def summarize_interface(keyword):\n",
    "    \"\"\"\n",
    "    Génère un résumé d'un document complet.\n",
    "    \n",
    "    Args:\n",
    "        keyword: Mot-clé pour identifier le document\n",
    "    \n",
    "    Returns:\n",
    "        Résumé du document ou message d'erreur\n",
    "    \"\"\"\n",
    "    if not keyword.strip():\n",
    "        return \"Veuillez entrer un mot-clé pour rechercher un document.\"\n",
    "    \n",
    "    try:\n",
    "        summary = summarize_document(keyword, llm, data_files, documents)\n",
    "        return f\"# Résumé du document\\n\\n{summary}\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la génération du résumé : {str(e)}\"\n",
    "\n",
    "\n",
    "def get_available_docs():\n",
    "    \"\"\"\n",
    "    Génère une liste formatée des documents disponibles.\n",
    "    \n",
    "    Returns:\n",
    "        Liste des documents en Markdown\n",
    "    \"\"\"\n",
    "    doc_list = \"# Documents disponibles\\n\\n\"\n",
    "    \n",
    "    # Grouper par catégorie\n",
    "    categories = {\n",
    "        'arxiv': ('Articles arXiv', [f for f in data_files if 'arxiv' in f]),\n",
    "        'autres': ('Autres articles', [f for f in data_files if 'autres_articles' in f]),\n",
    "        'patents': ('Brevets', [f for f in data_files if 'patents' in f])\n",
    "    }\n",
    "    \n",
    "    for title, docs in categories.values():\n",
    "        if docs:\n",
    "            doc_list += f\"## {title}\\n\\n\"\n",
    "            for doc in docs[:10]:\n",
    "                doc_list += f\"- `{Path(doc).name}`\\n\"\n",
    "            if len(docs) > 10:\n",
    "                doc_list += f\"- *... et {len(docs) - 10} autres*\\n\"\n",
    "            doc_list += \"\\n\"\n",
    "    \n",
    "    doc_list += f\"**Total : {len(data_files)} documents indexés**\"\n",
    "    \n",
    "    return doc_list\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTERFACE GRADIO\n",
    "# ============================================================================\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Système RAG Complet\") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # Système RAG - Assistant Documentaire Intelligent\n",
    "    \n",
    "    Explorez une base de connaissances scientifique avec un assistant IA avancé.\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        # ONGLET 1 : CHAT\n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        with gr.TabItem(\"Chat\", id=\"chat\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    chatbot = gr.Chatbot(\n",
    "                        label=\"Conversation\",\n",
    "                        height=450\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(\n",
    "                            label=\"Votre question\",\n",
    "                            placeholder=\"Posez votre question sur les documents...\",\n",
    "                            lines=2,\n",
    "                            scale=4,\n",
    "                            show_label=False\n",
    "                        )\n",
    "                        submit = gr.Button(\"Envoyer\", scale=1, variant=\"primary\")\n",
    "                    \n",
    "                    clear = gr.Button(\"Nouvelle conversation\", size=\"sm\")\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Paramètres\")\n",
    "                    \n",
    "                    k_docs = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=10,\n",
    "                        value=3,\n",
    "                        step=1,\n",
    "                        label=\"Nombre de documents\",\n",
    "                        info=\"Documents à consulter par requête\"\n",
    "                    )\n",
    "                    \n",
    "                    show_sources = gr.Checkbox(\n",
    "                        label=\"Afficher les sources\",\n",
    "                        value=True,\n",
    "                        info=\"Cite les documents utilisés\"\n",
    "                    )\n",
    "                    \n",
    "                    gr.Markdown(\"### Exemples\")\n",
    "                    gr.Examples(\n",
    "                        examples=[\n",
    "                            \"Qu'est-ce que le Multi-Agent Reinforcement Learning ?\",\n",
    "                            \"Comment mesurer la pression artérielle ?\",\n",
    "                            \"Parle-moi de la sécurité des smart grids\",\n",
    "                            \"Qu'est-ce que l'inflation targeting ?\",\n",
    "                        ],\n",
    "                        inputs=msg\n",
    "                    )\n",
    "            \n",
    "            # Gestion des événements du chat\n",
    "            def respond(message, chat_history, k, sources):\n",
    "                # Générer la réponse\n",
    "                bot_message = advanced_chatbot(message, chat_history, k, sources)\n",
    "                # Ajouter à l'historique au format dictionnaire\n",
    "                chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "                chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "                return \"\", chat_history\n",
    "            \n",
    "            msg.submit(respond, [msg, chatbot, k_docs, show_sources], [msg, chatbot], queue=False)\n",
    "            submit.click(respond, [msg, chatbot, k_docs, show_sources], [msg, chatbot], queue=False)\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        # ONGLET 2 : RECHERCHE\n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        with gr.TabItem(\"Recherche\", id=\"search\"):\n",
    "            gr.Markdown(\"### Recherche sémantique dans la base vectorielle\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                search_query = gr.Textbox(\n",
    "                    label=\"Requête de recherche\",\n",
    "                    placeholder=\"Entrez votre recherche (ex: quantum computing)...\",\n",
    "                    lines=2,\n",
    "                    scale=3\n",
    "                )\n",
    "                k_search = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=10,\n",
    "                    value=5,\n",
    "                    step=1,\n",
    "                    label=\"Nombre de résultats\",\n",
    "                    scale=1\n",
    "                )\n",
    "            \n",
    "            search_btn = gr.Button(\"Rechercher\", variant=\"primary\", size=\"lg\")\n",
    "            search_output = gr.Markdown(label=\"Résultats\", value=\"*Aucune recherche effectuée*\")\n",
    "            \n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    \"reinforcement learning applications\",\n",
    "                    \"blood pressure measurement techniques\",\n",
    "                    \"smart grid security threats\",\n",
    "                    \"quantum state learning complexity\",\n",
    "                ],\n",
    "                inputs=search_query\n",
    "            )\n",
    "            \n",
    "            search_btn.click(\n",
    "                search_interface,\n",
    "                inputs=[search_query, k_search],\n",
    "                outputs=search_output\n",
    "            )\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        # ONGLET 3 : RÉSUMÉ\n",
    "        # ─────────────────────────────────────────────────────────────────\n",
    "        with gr.TabItem(\"Résumé\", id=\"summary\"):\n",
    "            gr.Markdown(\"### Génération de résumés de documents complets\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    doc_keyword = gr.Textbox(\n",
    "                        label=\"Mot-clé du document\",\n",
    "                        placeholder=\"Ex: Multi-Agent, Blood_Pressure, Smart_Grid...\",\n",
    "                        lines=1\n",
    "                    )\n",
    "                    \n",
    "                    summarize_btn = gr.Button(\n",
    "                        \"Générer le résumé\",\n",
    "                        variant=\"primary\",\n",
    "                        size=\"lg\"\n",
    "                    )\n",
    "                    \n",
    "                    summary_output = gr.Markdown(\n",
    "                        label=\"Résumé généré\",\n",
    "                        value=\"*Aucun résumé généré*\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    docs_list = gr.Markdown(\n",
    "                        label=\"Documents disponibles\",\n",
    "                        value=get_available_docs()\n",
    "                    )\n",
    "                    \n",
    "                    refresh_docs = gr.Button(\"Rafraîchir\", size=\"sm\")\n",
    "            \n",
    "            summarize_btn.click(\n",
    "                summarize_interface,\n",
    "                inputs=doc_keyword,\n",
    "                outputs=summary_output\n",
    "            )\n",
    "            \n",
    "            refresh_docs.click(\n",
    "                get_available_docs,\n",
    "                outputs=docs_list\n",
    "            )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    ### Guide d'utilisation rapide\n",
    "    \n",
    "    | Onglet | Description | Utilisation |\n",
    "    |--------|-------------|-------------|\n",
    "    | **Chat** | Conversez avec l'assistant | Posez des questions, l'IA répond avec le contexte des documents |\n",
    "    | **Recherche** | Recherche sémantique | Trouvez les passages les plus pertinents dans la base |\n",
    "    | **Résumé** | Résumés de documents | Générez des synthèses structurées de documents entiers |\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch(\n",
    "    share=True,\n",
    "    debug=True,\n",
    "    show_error=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12249b4d",
   "metadata": {},
   "source": [
    "# Etape 4 : Evaluation\n",
    "\n",
    "## Exercice 8 : Evaluation\n",
    "\n",
    "VOIR [L6](./multi_agent_data/notebooks/L6/L6_évolution_Monitoring,_au_RAG_SQL,_à_l'Optimisation.ipynb) + Ajouter clé grok cellule 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
